Installing MCCE_Benchmarking for testing purposes:

Notation remarks:
  ">" denotes the command line prompt (whatever it may be in your profile);
  ">>" denotes the output of a command.

0. Preferably: cd $HOME

1. Install miniconda, if needed

  1.2 [Optional] Configure .condarc for installation channels & packages included with all new env, e.g.:
  ```
  # user_rc_path
  channel_priority: strict

  channels:
    - conda-forge
    - bioconda
    - newbooks

  auto_update_conda: true
  report_errors: true

  # disallow install of specific packages
  disallow:
    - anaconda

  # Add pip, wheel, and setuptools as deps of Python
  add_pip_as_python_dependency: true

  create_default_packages:
    - numpy
    - ipython
    - ipykernel
    - python-dotenv
    - matplotlib
    - pandas
    - pytest
    - black
    - pylint
    - flake8
  ```

2. Create a dedicated environment for all things mcce, e.g. 'mce':
   >conda create -n mce python 3.11

   - Activate the env:
     >conda activate mce  # -> new prompt: (mce) >

   - Install mcce
     * For Gunner Lab members: 
       mcce is already installed on the server, but it must be in you path
       variable; add this in your .bashrc, save & close the file, then source it:
       ```
       export PATH="/home/mcce/Stable-MCCE/bin:$PATH"
       ```
       (mce) >which mcce  # should return the added path

     * For all others, install the package:
       (mce) >conda install -c newbooks mcce
   

3. Install mcce_benchmark:
   (mce) >pip install git+https://github.com/GunnerLab/MCCE_Benchmarking.git#egg=mcce_benchmark

  3.1 Check that the installation created the main commands:
      (The commands will not be available outside your environment.)

      (mce) >which bench_expl_pkas
      >>~/miniconda3/envs/mce/bin/bench_expl_pkas
   
      The other ones are:
      (mce) >which bench_launchjob
      >>~/miniconda3/envs/mce/bin/bench_launchjob

      (mce) >which bench_analyze
      >>~/miniconda3/envs/mce/bin/bench_analyze

  3.2 Check the online help, e.g.:
      (mce) >bench_expl_pkas job_setup --help
      (mce) >bench_analyze expl_pkas --help


4. Run the first sub-command, job setup (includes scheduling step when run this way):
   (mce) >bench_expl_pkas setup_job -benchmarks_dir <some dir>


[5. Manually launching a batch (in case the scheduler fails):
    (mce) >bench_launchjob -benchmarks_dir <some dir> -n_active <n>   # n default: 10

 6. Wait ~ 30 mins and repeat #5 to update clean_pdbs/book.txt

    You can find out out many jobs are left to be submitted with this command:
    Note: replace 'mcce_benchmarks' if this is not the folder name you provided during setup:

    (mce) >to_submit=$(grep -v -e 'r$' -e 'c$' -e 'e$' mcce_benchmarks/clean_pdbs/book.txt | wc -l)
    (mce) >echo "$to_submit"

    (If -n_active equals the number of folders, the result should be 0.)
]

7. Analyze:
   The command line interface for analyzing a job setup via `bench_expl_pkas`, accepts two arguments:
   -benchmarks_dir [ -min_pct_complete ]. Note: The default for 'min_pct_complete' is 1 (100%): do 
   not modify until Issue 19 is closed.

   The first step of the analysis is to check that this threshold has been met. Incidentally, it's an
   easy way to check for how far long the job is.

   (mce) >bench_analyze expl_pkas -benchmarks_dir <some dir>


Please, open an issue with any problem installing or using the app at:
https://github.com/GunnerLab/MCCE_Benchmarking/issues

Thanks.
