nstalling MCCE_Benchmarking for testing purposes:

Notation remarks:
  ">" denotes the command line prompt (whatever it may be in your profile);
  ">>" denotes the output of a command.
  On a command line, "<x>" is an example of an argument value.


0. Preferably: cd $HOME

1. Install miniconda, if needed

  1.2 [Optional] Configure .condarc for installation channels & packages included with all new env, e.g.:
  ```
  # user_rc_path
  channel_priority: strict

  channels:
    - conda-forge
    - bioconda
    - newbooks

  auto_update_conda: true
  report_errors: true

  # disallow install of specific packages
  disallow:
    - anaconda

  # Add pip, wheel, and setuptools as deps of Python
  add_pip_as_python_dependency: true

  create_default_packages:
    - numpy
    - ipython
    - ipykernel
    - python-dotenv
    - matplotlib
    - pandas
    - pytest
    - black
    - pylint
    - flake8
  ```

2. Create a dedicated environment for all things mcce, e.g. 'mce':
   >conda create -n mce python=3.11   # the version part (=3.11) is optional, but not python

   - Activate the env:
     >conda activate mce  # => new prompt: (mce) >

   - Install mcce
     * For Gunner Lab members:
       [UPDATE, 03-01-2024:
       This is a MUST: the packaged 'Stable-MCCE' is missing a fortran library;
       See Issue 282, https://github.com/GunnerLab/Stable-MCCE/issues/282
       ]
       On the server, mcce is already installed, but it must be in your path variable; 
       Add the following line in your .bashrc, save & close the file, then source it:
       ```
       export PATH="/home/mcce/Stable-MCCE/bin:$PATH"
       ```

       (mce) > . ~/.bashrc   # source or 'dot' the file
       (mce) >which mcce     # should return the added path

     * For all others, install the package if Issue 282 is resolved:
       (mce) >conda install -c newbooks mcce
   

3. Install mcce_benchmark (right now from GitHub, not yet published on pypa):
   (mce) >pip install git+https://github.com/GunnerLab/MCCE_Benchmarking.git#egg=mcce_benchmark

  3.1 Check that the installation created the 4 cli commands:
      (The commands will NOT be available outside your environment.)

      (mce) >which bench_setup
      >>~/miniconda3/envs/mce/bin/bench_setup
   
      (mce) >which bench_launchjob
      >>~/miniconda3/envs/mce/bin/bench_launchjob

      (mce) >which bench_analyze
      >>~/miniconda3/envs/mce/bin/bench_analyze

      (mce) >which bench_compare
      >>~/miniconda3/envs/mce/bin/bench_compare


  3.2 Check the online help, e.g.:
      (mce) >bench_setup pkdb_pdbs --help
      (mce) >bench_setup user_pdbs --help


4. Use the setup command `bench_setup pkdb_pdbs` to create a set using the pdbs from pKaDBv1.
   Note: bench_setup creates the run script with the command line options for the mcce steps; 
         if none are given, the default run script is used.
   You can limit the number of curated proteins (folder) to setup using -n_pdbs.
   Give a low number for testing (max is 120, default):

   (mce) >bench_setup pkdb_pdbs -bench_dir <some/dir> -n_pdbs <2>

   * Job name:
   The default job name is 'default_run'; you can change it using -job_name.
   - If you use -job_name without amending any of the steps args, the default_run.sh file will be soft-linked as <job name>;
   - If you pass one or more non-default mcce steps parameters, then you MUST have a job name (& not 'default_run'):

   (mce) >bench_setup pkdb_pdbs -bench_dir <some/dir> -n_pdbs <2> -d 8 -job_name <foo_e8>

   * Sentinel file:
   The default sentinel_file is pK.out (means the run completed Step 4). It is part of script setup
   to ensure it is deleted prior to launching. You need to include it if you are not running all
   4 steps. Example running only steps 1 & 2:

   (mce) >bench_setup [pkdb_pdbs or user_pdbs] -bench_dir <some/dir> --s3_norun  --s4_norun -job_name <up_to_s2> -sentinel_file step2_out.pdb
   

   * Launch job right after setup:
   Launching a job with `bench_setup` is possible with the --launch flag:

   (mce) >bench_setup [pkdb_pdbs or user_pdbs] -bench_dir <some/dir> -d 8 -job_name <foo_e8> --launch

   In this case, you CANNOT review or amend the run script. You would not launch the job
   this way if, for instance, you want to add Step 6.
   TODO: Step 6 will be part of the script once a hydrogen-bond network comparison procedure is finalized.


5. Launch batches of jobs:
   5.0 Preferred way: use the automated scheduling to process all the pdbs in batches of size n_batch (default 10):
       (mce) >bench_setup launch -bench_dir <some/dir> -job_name <foo> -n_batch <4> [-sentinel_file x]  # needed if used in setup

  [
   5.1 Alternate way: launch a batch of size -n_batch (default: 10) at the command line:
       (mce) >bench_launchjob -bench_dir <some/dir> [ -job_name <same job name given in setup_job> ]

   5.2 Wait ~ 15 mins and repeat #5.1 to update clean_pdbs/book.txt
       You will need to repeat #5.2 until all the entries in book.txt show a status of 'e' or 'c'.

       You can find out out many jobs are left to be submitted with this command:

       (mce) >to_submit=$(grep -v -e 'r$' -e 'c$' -e 'e$' <some/dir>/RUNS/book.txt | wc -l)
       (mce) >echo $to_submit

       (If -n_batch > the number of folders, the result should be 0.)
  ]


6. Analyze (one set):
   The command line interface for analyzing a job setup via `bench_analyze [pkdb_pdbs or user_pdbs]`;
   requires 1 argument:  -bench_dir.
   The first step of the analysis checks that all the runs are completed. Incidentally, it's an
   easy way to check for how far along the processing is.

   * Output files & sub-commands:
    - user_pdbs: Collated pK.out and sum_crg.out, res and confs count files & related figures
    - pkdb_pdbs: Analysis additionally includes res and confs stats viz experimental pKa values
                 and related figures.


7. Compare two sets of runs, i.e. A/B testing (convention: B is reference):
   To compare two (completed) sets of runs, use `bench_compare`:
   Options: -di1, -dir2 -o (output folder), and two flags:
            --user_db_pdbs      # absence means 'pkdb_pdbs'
            --dir2_is_refset    # if used, --user_pdbs must NOT be present, e.g.:

   (mce) >bench_compare -dir1 <d1> dir2 parse.e4 --dir2_is_refset -o <d1/comp>


Please, open an issue with any problem installing or using the app at:
https://github.com/GunnerLab/MCCE_Benchmarking/issues

Thanks.
