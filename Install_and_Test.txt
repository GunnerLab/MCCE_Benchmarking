Installing MCCE_Benchmarking for testing purposes:

Notation remarks:
  ">" denotes the command line prompt (whatever it may be in your profile);
  ">>" denotes the output of a command.
  On a command line, "<x>" is an example of an argument value.


0. Preferably: cd $HOME

1. Install miniconda, if needed

  1.2 [Optional] Configure .condarc for installation channels & packages included with all new env, e.g.:
  ```
  # user_rc_path
  channel_priority: strict

  channels:
    - conda-forge
    - bioconda
    - newbooks

  auto_update_conda: true
  report_errors: true

  # disallow install of specific packages
  disallow:
    - anaconda

  # Add pip, wheel, and setuptools as deps of Python
  add_pip_as_python_dependency: true

  create_default_packages:
    - numpy
    - ipython
    - ipykernel
    - python-dotenv
    - matplotlib
    - pandas
    - pytest
    - black
    - pylint
    - flake8
  ```

2. Create a dedicated environment for all things mcce, e.g. 'mce':
   >conda create -n mce python=3.11   # the version part (=3.11) is optional, but not python

   - Activate the env:
     >conda activate mce  # => new prompt: (mce) >

   - Install mcce
     * For Gunner Lab members:
       [UPDATE, 03-01-2024:
       This is a MUST: the packaged 'Stable-MCCE' is missing a fortran library;
       See Issue 282, https://github.com/GunnerLab/Stable-MCCE/issues/282
       ]
       On the server, mcce is already installed, but it must be in your path variable; 
       Add the following line in your .bashrc, save & close the file, then source it:
       ```
       export PATH="/home/mcce/Stable-MCCE/bin:$PATH"
       ```

       (mce) > . ~/.bashrc   # source or 'dot' the file
       (mce) >which mcce     # should return the added path

     * For all others, install the package if Issue 282 is resolved:
       (mce) >conda install -c newbooks mcce
   

3. Install mcce_benchmark (right now from GitHub, not yet published on pypa):
   (mce) >pip install git+https://github.com/GunnerLab/MCCE_Benchmarking.git#egg=mcce_benchmark

  3.1 Check that the installation created the cli commands:
      (The commands will NOT be available outside your environment.)

      (mce) >which bench_expl_pkas
      >>~/miniconda3/envs/mce/bin/bench_expl_pkas
   
      The other ones are:
      (mce) >which bench_launchjob
      >>~/miniconda3/envs/mce/bin/bench_launchjob

      (mce) >which bench_analyze
      >>~/miniconda3/envs/mce/bin/bench_analyze

  3.2 Check the online help, e.g.:
      (mce) >bench_expl_pkas setup_job --help
      (mce) >bench_analyze expl_pkas --help


4. Run the first sub-command, setup_job (includes the script creation step):
   You can limit the number of curated proteins (folder) to setup using -n_pdbs.
   Give a low number for testing (max is 120, default):
   (mce) >bench_expl_pkas setup_job -benchmarks_dir <some/dir> -n_pdbs <2> -job_name <foo>


5. Launch batches of jobs:
   5.0 Preferred way: use the automated scheduling to process all the pdbs:
       (mce) >bench_expl_pkas launch_job -benchmarks_dir <some/dir> -job_name <foo> -n_batch <4>

  [
   5.1 Alternate way: launch a batch of size -n_batch (default: 10) at the command line:
       (mce) >bench_launchjob -benchmarks_dir <some/dir> [ -job_name <same job name given in setup_job> ]

   5.2 Wait ~ 15 mins and repeat #5.1 to update clean_pdbs/book.txt
       You will need to repeat #5.2 until all the entries in book.txt show a status of 'e' or 'c'.

       You can find out out many jobs are left to be submitted with this command:

       (mce) >to_submit=$(grep -v -e 'r$' -e 'c$' -e 'e$' <some/dir>/clean_pdbs/book.txt | wc -l)
       (mce) >echo $to_submit

       (If -n_batch > the number of folders, the result should be 0.)
  ]

6. Analyze:
   The command line interface for analyzing a job setup via `bench_expl_pkas`, accepts two arguments:
   -benchmarks_dir [ -min_pct_complete ]. Note: The default for 'min_pct_complete' is 1 (100%): do 
   not modify until Issue 19 (https://github.com/GunnerLab/MCCE_Benchmarking/issues/19) is closed.

   The first step of the analysis is to check that this threshold has been met. Incidentally, it's an
   easy way to check for how far long the job is.

   (mce) >bench_analyze expl_pkas -benchmarks_dir <some/dir>

   Note: [FUTURE] bench_analyze will have a second subcommand, likely 'mcce_runs', when the benchmarking
         cli for benchmarking mcce runs is completed, likely 'bench_mcce_runs'.


Please, open an issue with any problem installing or using the app at:
https://github.com/GunnerLab/MCCE_Benchmarking/issues

Thanks.
