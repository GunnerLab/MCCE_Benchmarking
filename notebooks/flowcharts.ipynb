{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Flowcharts for MCCE_Benchmarking:\n",
    "\n",
    "Note: GitHub markdown apparently cannot render the mermaid graphs despite saying it does.\n",
    "\n",
    "\n",
    "## Purpose:\n",
    "\n",
    "To allow benchmarking viz the curated, experimental pKa database (v1). **DONE**.  \n",
    "To allow benchmarking of mcce runs. **TODO**  \n",
    "\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# 03-05-2024\n",
    "---\n",
    "# Update post 3/4/24 group meeting on benchmarking\n",
    "  * Variable names change:\n",
    "    - n_active -> n_batch\n",
    "    - job_setup -> setup_job (in docuemntation)\n",
    "  * Added missing scipy requirement (needed for mcce, not mcce_benchmarking per se)\n",
    "\n",
    "\n",
    "## Completion status:\n",
    " * Entry point 'bench_expl_pkas' with sub-commands 'setup_job' and 'launch_job': DONE\n",
    " * Entry point 'bench_mcce_runs': 30%\n",
    "\n",
    "---\n",
    "## Flowchart for the current entry points (EPs) of `mcce_benchmark`:\n",
    "\n",
    "```mermaid\n",
    "%%{init: {\"flowchart\": {\"htmlLabels\": false}} }%%\n",
    "flowchart TD\n",
    "    exp(\"`EP:  _**bench_expl_pkas**_\n",
    "    <br>\n",
    "    Status: **DONE**\n",
    "    <br>\n",
    "    sub-cmd 1: _setup_job_\n",
    "    sub-cmd 2: _launch_job_`\"\n",
    "    ) -- 1--> sb1[setup_job]\n",
    "    exp -- 2 --> sb2[launch_job]\n",
    "    sb1 -- Input: 'benchmarks_dir' --> dir1[\\-benchmarks_dir exists?/]\n",
    "    dir1 -->|N| ng1[mkdir 'benchmarks_dir']\n",
    "    dir1 -->|Y| dosb1[Actions performed:\n",
    "    1. Data setup\n",
    "    2. Automated  cron scheduling\n",
    "    ]\n",
    "    ng1 --> dosb1\n",
    "    sb2 --> dosb2[Create scheduling via crontab]\n",
    "    dosb2 -- Monitor % completion --> cmd1[>grep -i 'completed' benchmarks_dir/cron_job_name.log]\n",
    "\n",
    "    batch(\"`EP:  _**bench_launchjob**_\n",
    "    (used in crontab)\n",
    "    <br>\n",
    "    Status: **DONE**\n",
    "    <br>\n",
    "    Submit ONE batch of size -n_batch\n",
    "    using sh script`\"\n",
    "    ) -- Pre-requisites:\n",
    "    A setup as in `bench_expl_pkas setup_job`:\n",
    "     - Same folder structure\n",
    "     - 'default_run.sh' or 'job_name.sh' script\n",
    "        if job_name was used. --> wait(Wait x min)\n",
    "     wait -->|Monitor % completion| cmd2[>grep -i 'completed' ../benchmark.log]\n",
    "     cmd2 -->|Repeat until all jobs are completed| batch\n",
    "\n",
    "    anz(\"`EP: _**bench_analyze**_\n",
    "    <br>\n",
    "    Status: **50% done**\n",
    "    <br>\n",
    "    sub-cmd 1: expl_pkas\n",
    "    sub-cmd 2: mcce_runs`\"\n",
    "    ) -->ok[\\Are all the jobs completed?/]\n",
    "    ok -->|N| nogo1(Check again later!)\n",
    "    ok -->|Y|asb1[1. expl_pkas]\n",
    "    ok -->|Y|asb2[2. mcce_runs TODO]\n",
    "    asb1 -->|Pre-requisites:\n",
    "    Same folder structure as in\n",
    "    `bench_expl_pkas setup_job`| dir2[\\-benchmarks_dir exists?/]\n",
    "    dir2 -->|Y|doasb1[Create all_pkas.out\n",
    "    Output report files\n",
    "    and figures in\n",
    "    benchmarks_dir/analysis]\n",
    "    dir2 -->|N| npgp2(Oops! Typo? Wrong dir?)\n",
    "    asb2 -->|Pre-requisites:\n",
    "    completed runs| doasb2[\"`**TBD**:\n",
    "    diff of pK.out, sum_crg.out`\"]\n",
    "```\n",
    "```mermaid\n",
    "%%{init: {\"flowchart\": {\"htmlLabels\": false}} }%%\n",
    "\n",
    "flowchart LR\n",
    "subgraph sg0 [\"`**Note:**`\"]\n",
    "    o[\"Because the folder structure and and pdbs will be different:\"]\n",
    "    style o fill:#fff,stroke:#f66,stroke-width:1px\n",
    "\n",
    "    subgraph sg1 [\"expl_pKas: pH only, pkadb pdbs & setup\"]\n",
    "    direction LR\n",
    "      a(\"`**bench_expl_pkas**`\") ==> b[\"`_bench_analyze_ **expl_pkas**`\"]\n",
    "    end\n",
    "    subgraph sg2 [\"mcce_runs**: pH, Eh, different setup...\"]\n",
    "    direction LR\n",
    "      c(\"`**bench_mcce_runs**`\") ==>d[\"`_bench_analyze_ **mcce_runs**`\"]\n",
    "    end\n",
    "\n",
    "end\n",
    "```"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "# not like that!\n",
    "\n",
    "## Flowchart for the furtue EP of `mcce_benchmark`: benchmarking mcce runs\n",
    "\n",
    "```mermaid\n",
    "%%{init: {\"flowchart\": {\"htmlLabels\": false}} }%%\n",
    "flowchart TD\n",
    "    mc(\"`EP: _**bench_mcce_runs**_`\") -->|Inputs:\n",
    "        2 completed runs:\n",
    "        'new_calc_dir', 'reference_dir';\n",
    "        'titr_type'| ref{Which 'reference_dir'?}\n",
    "        ref -->|ref dir is e.g. 'parse.e4' from pKaDB\n",
    "        Applicable only to pH titrations| comp2[Use 'all_pkas.e4' file for comparison]\n",
    "        comp2 --> mcpka[Analysis outputs:\n",
    "        * Matched, then diffed pKa values\n",
    "        * Plot new vs ref for all numeric fields in pK.out]\n",
    "        ref -->|ref dir is another mcce output dir\n",
    "        ASSUMED: runs of same prot| rptmc[Analysis outputs:\n",
    "        * Diffed pK.out\n",
    "        * Residue stats\n",
    "        * Plot new vs ref for all numeric fields in pK.out\n",
    "        * Anything else?]\n",
    "        mcpka -.-> note[Problem:\n",
    "        Analysis will depend on the\n",
    "        contents of 'parse.e4' dir:\n",
    "        full output, partial\n",
    "        or just 'all_pkas.e4' file?]\n",
    "        rptmc -.-> note\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "# 2-14-2024\n",
    "---\n",
    "# Refactoring MCCE benchmark cli ([Issue 16](https://github.com/GunnerLab/MCCE_Benchmarking/issues/16))\n",
    "\n",
    "## Purpose:\n",
    "To allow benchmarking viz the curated, experimental pKa database (v1) or viz MCCE runs (two runs with completed step4).\n",
    "\n",
    "## Completion status:\n",
    "The refactoring is not complete yet as it entails creating two different tracks (or entry points), along with automated tests.\n",
    "* 'experimental_pkas': 80%\n",
    "* 'mcce_runs': 30%\n",
    "\n",
    "## What the command line commands would be (w/o renaming main EP):\n",
    "### Entry point 'experimental_pkas':\n",
    " 1. Setup the pdbs folder and files for the user. NO GO: `benchmarks_dir` already exists\n",
    " ```\n",
    " # No input means `benchmarks_dir` = ./mcce_benchmarks:\n",
    "\n",
    " >mccebench experimental_pkas benchmark_setup\n",
    "\n",
    "\n",
    " # With input:\n",
    "\n",
    " >mccebench experimental_pkas benchmark_setup -benchmarks_dir <different name>\n",
    "\n",
    "\n",
    " ```\n",
    " 2. Analysis (default for `pct_complete` not yet determined):\n",
    " ```\n",
    " >mccebench experimental_pkas analyze -pct_complete <float> [-benchmarks_dir: optional if default was used in `benchmark_setup`]\n",
    " ```\n",
    "\n",
    "### Entry point 'mcce_runs':\n",
    "NOTE: I think there is no need for the -eps parameter since we would want to compare, say e8 vs e4 (default); Updated the flowchart accordingly.  \n",
    "NO GO: User selected -titr_type = 'eh' with -reference_dir = 'parse.e4': this reference set is for ph titrations.\n",
    "```\n",
    " >mccebench mcce_runs -new_calc_dir <new>, -reference_dir <ref> -titr_type <ph or eh>\n",
    "\n",
    "\n",
    "```\n",
    "\n",
    "---\n",
    "## Flowchart for the two entry points:\n",
    "\n",
    "```mermaid\n",
    "graph TD\n",
    "    exp[Entry point: 'experimental_pkas'\n",
    "    default script: dry prot, 4 steps w/mfe] -->|Input: 'benchmards_dir'| dir{dir exists?}\n",
    "    dir -->|N| ng1[STOP:\n",
    "    Rename existing dir or\n",
    "    Change 'benchmarks_dir']\n",
    "    dir -->|Y| sub1{Subcommand choice:\n",
    "    1. 'benchmark_setup'\n",
    "    2. 'analyze'}\n",
    "    sub1 -->| 'benchmark_setup' |do1[Actions performed:\n",
    "    1. Data setup\n",
    "    2. Scheduling setup\n",
    "    3. Launch]\n",
    "    sub1 -->| 'analyze' | prob1[Problem:\n",
    "    Are there enough completed runs?\n",
    "    Implement with a 'percentage' user input?\n",
    "    => Function needed as initial check\n",
    "    to obtain the completed entries in\n",
    "    the book file & launch the analysis\n",
    "    if % is met.]\n",
    "    prob1 --> runs1{Enough completed runs?}\n",
    "    runs1 -->| Y | rpt1[Final outputs:\n",
    "    * all_pkas file\n",
    "    * Matched pKas file\n",
    "    * Residue stats\n",
    "    * Conformers throughput per step\\n using runtimes & conformer counts\n",
    "    * Plots\n",
    "    * Anything else?]\n",
    "    runs1 --> | N | msg1(Try '>experimental_pkas analyze' later)\n",
    "\n",
    "    mc[Entry point: 'mcce_runs'] -->|Inputs:\n",
    "    2 completed runs:\n",
    "    'new_calc_dir', 'reference_dir';\n",
    "    'titr_type'| ref{Which 'reference_dir'?}\n",
    "    ref -->|ref dir is e.g. 'parse.e4' from pKaDB\n",
    "    Applicable only to pH titrations| comp2[Use 'all_pkas.e4' file for comparison]\n",
    "    comp2 --> mcpka[Analysis outputs:\n",
    "    * Matched, then diffed pKa values\n",
    "    * Plot new vs ref for all numeric fields in pK.out]\n",
    "    ref -->|ref dir is another mcce output dir\n",
    "    ASSUMED: runs of same prot| rptmc[Analysis outputs:\n",
    "    * Diffed pK.out\n",
    "    * Residue stats\n",
    "    * Plot new vs ref for all numeric fields in pK.out\n",
    "    * Anything else?]\n",
    "    mcpka -.-> note[Problem:\n",
    "    Analysis will depend on the\n",
    "    contents of 'parse.e4' dir:\n",
    "    full output, partial\n",
    "    or just 'all_pkas.e4' file?]\n",
    "    rptmc -.-> note\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:mce]",
   "language": "python",
   "name": "conda-env-mce-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
