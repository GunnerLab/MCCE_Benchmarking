{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Run the first 2 code cells without modifications_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python ver: 3.11.5 | packaged by conda-forge | (main, Aug 27 2023, 03:34:09) [GCC 12.3.0]\n",
      "Python env: mce\n",
      "Currrent dir: /home/cat/projects/MCCE_Benchmarking/notebooks\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "from IPython.display import Markdown #, IFrame\n",
    "# for presentations:\n",
    "#display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "#pd.set_option(\"display.max_colwidth\", 200)\n",
    "import matplotlib as mpl\n",
    "from matplotlib import pyplot as plt\n",
    "plt.ion()\n",
    "plt.style.use('seaborn-v0_8-muted')\n",
    "from pprint import pprint as ptp\n",
    "\n",
    "import sys\n",
    "from pathlib import Path\n",
    "print('Python ver: {}\\nPython env: {}'.format(sys.version, Path(sys.prefix).name))\n",
    "print('Currrent dir: {}\\n'.format(Path.cwd()))\n",
    "\n",
    "\n",
    "def add_to_sys_path(this_path, up=False):\n",
    "    \"\"\"\n",
    "    Prepend this_path to sys.path.\n",
    "    If up=True, path refers to parent folder (1 level up).\n",
    "    \"\"\"\n",
    "\n",
    "    if up:\n",
    "        newp = str(Path(this_path).parent)\n",
    "    else:\n",
    "        newp = str(Path(this_path))\n",
    "    if newp not in sys.path:\n",
    "        sys.path.insert(1, newp)\n",
    "        print('Path added to sys.path: {}'.format(newp))\n",
    "\n",
    "\n",
    "def fdir(obj, start_with_str='_', exclude=True):\n",
    "    \"\"\"Filtered dir() for method discovery.\"\"\"\n",
    "    return [d for d in dir(obj) if not d.startswith(start_with_str) == exclude]\n",
    "\n",
    "def despine(which=['top','right']):\n",
    "    \"\"\"which ([str])): 'left','top','right','bottom'.\"\"\"\n",
    "\n",
    "    ax = plt.gca()\n",
    "    for side in which:\n",
    "        ax.spines[side].set_visible(False)\n",
    "    return\n",
    "\n",
    "def md_width_comment(w:int=120) -> str:\n",
    "    \"\"\"Width guide for composing md documents.\"\"\"\n",
    "    return f\"<!-- dotted line width = {w}\\n{'.'*w}-->\"\n",
    "\n",
    "\n",
    "# autoreload extension\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "notebooks_dir = Path.cwd()\n",
    "add_to_sys_path(notebooks_dir, up=True)\n",
    "notebooks_dir"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "# OUTDATED!\n",
    "\n",
    "```mermaid\n",
    "flowchart TD\n",
    "    exp[Entry point: 'experimental_pkas'\n",
    "    default script: dry prot, 4 steps w/mfe] -->|Input: 'benchmards_dir'| dir{dir exists?}\n",
    "    dir -->|N| ng1[STOP:\n",
    "    Rename existing dir or\n",
    "    Change 'benchmarks_dir']\n",
    "    dir -->|Y| sub1{Subcommand choice:\n",
    "    1. 'benchmark_setup'\n",
    "    2. 'analyze'}\n",
    "    sub1 -->| 'benchmark_setup' |do1[Actions performed:\n",
    "    1. Data setup\n",
    "    2. Scheduling setup\n",
    "    3. Launch]\n",
    "    sub1 -->| 'analyze' | prob1[Problem:\n",
    "    Are there enough completed runs?\n",
    "    Implement with a 'percentage' user input?\n",
    "    => Function needed as initial check\n",
    "    to obtain the completed entries in\n",
    "    the book file & launch the analysis\n",
    "    if % is met.]\n",
    "    prob1 --> runs1{Enough completed runs?}\n",
    "    runs1 -->| Y | rpt1[Final outputs:\n",
    "    * all_pkas file\n",
    "    * Matched pKas file\n",
    "    * Residue stats\n",
    "    * Conformers throughput per step\\n using runtimes & conformer counts\n",
    "    * Plots\n",
    "    * Anything else?]\n",
    "    runs1 --> | N | msg1(Try '>experimental_pkas analyze' later)\n",
    "\n",
    "    mc[Entry point: 'mcce_runs'] -->|Inputs:\n",
    "    2 completed runs:\n",
    "    'new_calc_dir', 'reference_dir';\n",
    "    'titr_type', 'eps'| ref{Which 'reference_dir'?}\n",
    "    ref -->|ref dir is e.g. 'parse.e4' from pKaDB\n",
    "    Applicable only to pH titrations| comp2[Use 'all_pkas.e4' file for comparison]\n",
    "    comp2 --> mcpka[Analysis outputs:\n",
    "    * Matched, then diffed pKa values\n",
    "    * Plot new vs ref for all numeric fields in pK.out]\n",
    "    ref -->|ref dir is another mcce output dir\n",
    "    ASSUMED: runs of same prot| rptmc[Analysis outputs:\n",
    "    * Diffed pK.out\n",
    "    * Residue stats\n",
    "    * Plot new vs ref for all numeric fields in pK.out\n",
    "    * Anything else?]\n",
    "    mcpka -.-> note[Problem:\n",
    "    Analysis will depend on the\n",
    "    contents of 'parse.e4' dir:\n",
    "    full output, partial\n",
    "    or just 'all_pkas.e4' file?]\n",
    "    rptmc -.-> note\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mcce_benchmark import APP_NAME, BENCH\n",
    "from mcce_benchmark import DEFAULT_DIR, N_BATCH, N_PDBS, ENTRY_POINTS, OUT_FILES, MCCE_OUTPUTS, ANALYZE_DIR\n",
    "\n",
    "from mcce_benchmark import USER_PRFX, USER_ENV, CONDA_PATH\n",
    "from mcce_benchmark import Pathok\n",
    "\n",
    "import subprocess\n",
    "from typing import Union\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "print(BENCH)\n",
    "print(f\"{USER_PRFX = }\\n{USER_ENV = }\\n{CONDA_PATH = }\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/home/cat/projects/MCCE_Benchmarking/notebooks/_tmp')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_dir = PosixPath('/home/cat/projects/mcce_benchmarks_test')\n",
      "benchmarks_dir = PosixPath('/home/cat/projects/mcce_benchmarks_test')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PosixPath('/home/cat/projects/mcce_benchmarks_test/clean_pdbs')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# completed runs:\n",
    "here = Path.cwd()\n",
    "here\n",
    "\n",
    "test_dir = Pathok(Path.cwd().parent.parent.joinpath(\"mcce_benchmarks_test\"))\n",
    "print(f\"{test_dir = }\")\n",
    "\n",
    "benchmarks_dir = Pathok(Path.cwd().parent.parent.joinpath(\"mcce_benchmarks_test\"))\n",
    "print(f\"{benchmarks_dir = }\")\n",
    "\n",
    "pdbs = benchmarks_dir.joinpath(BENCH.CLEAN_PDBS)\n",
    "pdbs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "# Refset prep:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "---\n",
    "\n",
    "# prep download copy of completed refset from isis; keep pK.out until pkanalysis run on folder\n",
    "\n",
    "```\n",
    "total 184\r\n",
    "-rw-r--r--   1 cat cat 173762 Feb 16 10:08 WT_pkas.csv\r\n",
    "-rw-r--r--   1 cat cat    483 Feb 16 10:13 benchmark.log\r\n",
    "drwxr-xr-x 122 cat cat   4096 Feb 16 10:13 clean_pdbs\r\n",
    "-rw-r--r--   1 cat cat   2763 Feb 16 10:08 proteins\n",
    "```\n",
    "# cleanup of mcce_benchmarks copy from isis:\r\n",
    "\r\n",
    "1. get\r\n",
    "scp -r cchenal@134.74.25.124/:~projects/mcce_benchm\n",
    "arks/ .m t files exceptbe  kept:\r\n",
    "  -rucn.prm. rord\r\n",
    "  -  run.lp2_out.pdb\n",
    "  - new.tpl oprot.tpl  ave43 pr\n",
    " h\n",
    "  one)    - pK.out :: until anal\n",
    "ison():.tsv"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "# Use jmao's benchmark folder as testing data\n",
    "0. Rename clean_pdb/ -> clean_pdbs/            [x]\n",
    "1. get list of full runs                       [x]\n",
    "(base) cat@LABTOP:~/projects/benchmark$\n",
    "find clean_pdbs/*/ -type f -name \"pK.out\"\n",
    "\n",
    "2. match dir & copy all files except prot.pdb to mcce_benchmarks_test\n",
    "\n",
    "\n",
    "# To have complete runs in canonical folder structure:\n",
    "\n",
    "# jmao benchamrk folder\n",
    "jmao_dir = Path.cwd().parent.parent.joinpath(\"benchmark\", \"e08_calc\") #-> like clean_pdbs\n",
    "print(f\"{jmao_dir = }\", jmao_dir.exists())\n",
    "\n",
    "#list_complete_runs(jmao_dir, like_clean_pdbs=True)\n",
    "#audit.cp_completed_runs(jmao_dir, test_dir)\n",
    "\n",
    "#audit.list_complete_runs(test_dir)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "cleanup.prep_refset(benchmarks_dir, keep_files=files_retained)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "pkanalysis.collate_all_pkas(test_dir)\n",
    "pkanalysis.all_pkas_df(test_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import logging\n",
    "logger = logging.getLogger('cli')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Test cli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "from argparse import ArgumentParser, ArgumentError, RawDescriptionHelpFormatter, Namespace as argNamespace\n",
    "from mcce_benchmark import cli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "clilogr = cli.logger\n",
    "clilogr.hasHandlers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: \n",
      "bench_expl_pkas <+ 1 sub-command: setup_job or launch_job> <related args>\n",
      "\n",
      "Examples:\n",
      "1. setup_job: Data & script setup:\n",
      "   - Minimal input: value for -benchmarks_dir option:\n",
      "     >bench_expl_pkas setup_job -benchmarks_dir <folder name>\n",
      "\n",
      "   - Using non-default option(s):\n",
      "     >bench_expl_pkas setup_job -benchmarks_dir <folder name> -d 8\n",
      "\n",
      "2. launch_job: Launch runs:\n",
      "   - Minimal input: value for -benchmarks_dir option:\n",
      "     >bench_expl_pkas launch_job -benchmarks_dir <folder name>\n",
      "\n",
      "   - Using non-default option(s):\n",
      "     >bench_expl_pkas launch_job -benchmarks_dir <folder name> -n_active <jobs to maintain>\n",
      "     >bench_expl_pkas launch_job -benchmarks_dir <folder name> -job_name <my_job_name> -sentinel_file step2_out.pdb\n",
      "\n",
      "Description:\n",
      "Launch a MCCE benchmarking job using curated structures from the pKa Database v1.\n",
      "\n",
      "The main command is bench_expl_pkas along with one of 2 sub-commands:\n",
      "- Sub-command 1: setup_job: setup the dataset and run script to run mcce steps 1 through 4;\n",
      "- Sub-command 2: launch_job: launch a batch of jobs;\n",
      "\n",
      "options:\n",
      "  -h, --help            show this help message and exit\n",
      "\n",
      "bench_expl_pkas sub-commands:\n",
      "  Sub-commands of MCCE benchmarking cli.\n",
      "\n",
      "  {setup_job,launch_job}\n",
      "                        The 3 choices for the benchmarking process: 1) Setup\n",
      "                        data & run-script: {SUB_CMD1} 2) Batch-run mcce steps\n",
      "                        1 through 4: {SUB_CMD2}\n",
      "    setup_job           Sub-command for setting up <benchmarks_dir>/clean_pdbs\n",
      "                        folder & job_name_run.sh script, e.g.:\n",
      "                        >bench_expl_pkas setup_job -benchmarks_dir <folder\n",
      "                        name>\n",
      "    launch_job          Sub-command for launching a batch of jobs, e.g.:\n",
      "                        >bench_expl_pkas launch_job -benchmarks_dir <folder\n",
      "                        name> -n_active 15 Note: if provided, the value for\n",
      "                        the -job_name option must match the one used in\n",
      "                        `setup_job`.\n",
      "\n",
      "Post an issue for all errors and feature requests at:\n",
      "https://github.com/GunnerLab/MCCE_Benchmarking/issues\n"
     ]
    }
   ],
   "source": [
    "cli_parser = cli.bench_parser()\n",
    "\n",
    "cli_parser.print_help()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Test help msg\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## cli: None input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Description:\n",
      "Launch a MCCE benchmarking job using curated structures from the pKa Database v1.\n",
      "\n",
      "The main command is bench_expl_pkas along with one of 2 sub-commands:\n",
      "- Sub-command 1: setup_job: setup the dataset and run script to run mcce steps 1 through 4;\n",
      "- Sub-command 2: launch_job: launch a batch of jobs;\n",
      "\n",
      "\n",
      "bench_expl_pkas <+ 1 sub-command: setup_job or launch_job> <related args>\n",
      "\n",
      "Examples:\n",
      "1. setup_job: Data & script setup:\n",
      "   - Minimal input: value for -benchmarks_dir option:\n",
      "     >bench_expl_pkas setup_job -benchmarks_dir <folder name>\n",
      "\n",
      "   - Using non-default option(s):\n",
      "     >bench_expl_pkas setup_job -benchmarks_dir <folder name> -d 8\n",
      "\n",
      "2. launch_job: Launch runs:\n",
      "   - Minimal input: value for -benchmarks_dir option:\n",
      "     >bench_expl_pkas launch_job -benchmarks_dir <folder name>\n",
      "\n",
      "   - Using non-default option(s):\n",
      "     >bench_expl_pkas launch_job -benchmarks_dir <folder name> -n_active <jobs to maintain>\n",
      "     >bench_expl_pkas launch_job -benchmarks_dir <folder name> -job_name <my_job_name> -sentinel_file step2_out.pdb\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cmd = None\n",
    "cli.bench_cli(cmd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Description:\n",
      "Launch a MCCE benchmarking job using curated structures from the pKa Database v1.\n",
      "\n",
      "The main command is bench_expl_pkas along with one of 2 sub-commands:\n",
      "- Sub-command 1: setup_job: setup the dataset and run script to run mcce steps 1 through 4;\n",
      "- Sub-command 2: launch_job: launch a batch of jobs;\n",
      "\n",
      "\n",
      "bench_expl_pkas <+ 1 sub-command: setup_job or launch_job> <related args>\n",
      "\n",
      "Examples:\n",
      "1. setup_job: Data & script setup:\n",
      "   - Minimal input: value for -benchmarks_dir option:\n",
      "     >bench_expl_pkas setup_job -benchmarks_dir <folder name>\n",
      "\n",
      "   - Using non-default option(s):\n",
      "     >bench_expl_pkas setup_job -benchmarks_dir <folder name> -d 8\n",
      "\n",
      "2. launch_job: Launch runs:\n",
      "   - Minimal input: value for -benchmarks_dir option:\n",
      "     >bench_expl_pkas launch_job -benchmarks_dir <folder name>\n",
      "\n",
      "   - Using non-default option(s):\n",
      "     >bench_expl_pkas launch_job -benchmarks_dir <folder name> -n_active <jobs to maintain>\n",
      "     >bench_expl_pkas launch_job -benchmarks_dir <folder name> -job_name <my_job_name> -sentinel_file step2_out.pdb\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cmd = []\n",
    "cli.bench_cli(cmd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## cli: 1 input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['setup_job']"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sub-command for setting up <benchmarks_dir>/clean_pdbs folder & job_name_run.sh script, e.g.:\n",
      ">bench_expl_pkas setup_job -benchmarks_dir <folder name>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cmd = f\"{cli.SUB_CMD1}\".split()\n",
    "cmd\n",
    "cli.bench_cli(cmd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sub-command for launching a batch of jobs, e.g.:\n",
      ">bench_expl_pkas launch_job -benchmarks_dir <folder name> -n_active 15\n",
      "Note: if provided, the value for the -job_name option must match the one used in `setup_job`.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cmd = f\"{cli.SUB_CMD2}\".split() #benchmarks_dir {benchmarks_dir} -job_name {job} -d 8\".split()\n",
    "\n",
    "cli.bench_cli(cmd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## cli: 2 with help"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['setup_job', '-h']"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Description:\n",
      "Launch a MCCE benchmarking job using curated structures from the pKa Database v1.\n",
      "\n",
      "The main command is bench_expl_pkas along with one of 2 sub-commands:\n",
      "- Sub-command 1: setup_job: setup the dataset and run script to run mcce steps 1 through 4;\n",
      "- Sub-command 2: launch_job: launch a batch of jobs;\n",
      "\n",
      "\n",
      "bench_expl_pkas <+ 1 sub-command: setup_job or launch_job> <related args>\n",
      "\n",
      "Examples:\n",
      "1. setup_job: Data & script setup:\n",
      "   - Minimal input: value for -benchmarks_dir option:\n",
      "     >bench_expl_pkas setup_job -benchmarks_dir <folder name>\n",
      "\n",
      "   - Using non-default option(s):\n",
      "     >bench_expl_pkas setup_job -benchmarks_dir <folder name> -d 8\n",
      "\n",
      "2. launch_job: Launch runs:\n",
      "   - Minimal input: value for -benchmarks_dir option:\n",
      "     >bench_expl_pkas launch_job -benchmarks_dir <folder name>\n",
      "\n",
      "   - Using non-default option(s):\n",
      "     >bench_expl_pkas launch_job -benchmarks_dir <folder name> -n_active <jobs to maintain>\n",
      "     >bench_expl_pkas launch_job -benchmarks_dir <folder name> -job_name <my_job_name> -sentinel_file step2_out.pdb\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cmd = f\"{cli.SUB_CMD1} -h\".split() #benchmarks_dir {benchmarks_dir} -job_name {job} -d 8\".split()\n",
    "cmd\n",
    "#args = cli_parser.parse_args(cmd)\n",
    "\n",
    "cli.bench_cli(cmd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## cli: setup with n_pdbs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mcce_benchmark import custom_sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO]: mcce_benchmark.cli, <module>:\n",
      "\t\n",
      "START\n",
      "----------------------------------------------------------------------\n",
      "2024-02-28 13:41:41 - USER = 'cat' - User envir: mce\n",
      "APP VER: (0, 1, 0, 'dev5', 'g2095b87.d20240226')\n",
      "APP DEFAULTS:\n",
      "Globals: MCCE_EPS = 4; N_ACTIVE = 10\n",
      "Default resource names:\n",
      "DEFAULT_DIR = 'mcce_benchmarks' : default benchmarking folder name\n",
      "BENCH.CLEAN_PDBS = 'clean_pdbs' : fixed\n",
      "BENCH.Q_BOOK = 'book.txt' : jobs bookkeeping file\n",
      "BENCH.DEFAULT_JOB = 'default_run' (-> default_run.sh script in clean_pdbs/)\n",
      "BENCH.BENCH_PARSE_E4 = PosixPath('/home/cat/projects/MCCE_Benchmarking/mcce_benchmark/data/refsets/parse.e4') : Current reference set\n",
      "N_PDBS = 120 : number of pdbs in the dataset\n",
      "Default analysis output file names (fixed):\n",
      "OUT_FILES.MATCHED_PKAS_FILE.name = 'MATCHED_PKAS_FILE'\n",
      "OUT_FILES.ALL_PKAS_FILE.name = 'ALL_PKAS_FILE'\n",
      "OUT_FILES.CONF_COUNTS.name = 'CONF_COUNTS'\n",
      "OUT_FILES.RES_COUNTS.name = 'RES_COUNTS'\n",
      "OUT_FILES.RUN_TIMES.name = 'RUN_TIMES'\n",
      "OUT_FILES.CONFS_PER_RES.name = 'CONFS_PER_RES'\n",
      "OUT_FILES.CONFS_THRUPUT.name = 'CONFS_THRUPUT'\n",
      "----------------------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['setup_job', '-benchmarks_dir', '../foo_dir', '-n_pdbs', '2']"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ben_dir = \"../foo_dir\"\n",
    "cmd = f\"{cli.SUB_CMD1} -benchmarks_dir {ben_dir} -n_pdbs 2\".split() # -job_name {job} -d 8\".split()\n",
    "cmd\n",
    "args = cli_parser.parse_args(cmd)\n",
    "\n",
    "all_default = custom_sh.all_opts_are_defaults(args)\n",
    "all_default\n",
    "\n",
    "#cli.bench_cli(cmd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['launch_job', '-benchmarks_dir', '../foo_dir']"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO]: mcce_benchmark.cli, bench_launch_batch:\n",
      "\tbench_expl_pkas args:\n",
      "{'subparser_name': 'launch_job',\n",
      " 'benchmarks_dir': PosixPath('/home/cat/projects/MCCE_Benchmarking/notebooks/foo_dir'),\n",
      " 'job_name': 'default_run',\n",
      " 'n_active': 10,\n",
      " 'sentinel_file': 'pK.out',\n",
      " 'func': <function mcce_benchmark.cli.bench_launch_batch(args: argparse.Namespace) -> None>}\n",
      "\n",
      "[INFO]: mcce_benchmark.cli, bench_launch_batch:\n",
      "\tScript contents prior to launch:\n",
      "#!/bin/bash\n",
      "\n",
      "step1.py --dry prot.pdb\n",
      "step2.py -d 4\n",
      "step3.py -d 4\n",
      "step4.py --xts\n",
      "\n",
      "sleep 10\n",
      "\n",
      "[INFO]: mcce_benchmark.cli, bench_launch_batch:\n",
      "\tSubmiting batch of jobs.\n",
      "[INFO]: mcce_benchmark.scheduling, create_cron_sh:\n",
      "\tCreated script for crontab 'crontab_default_run_sh' in /home/cat/projects/MCCE_Benchmarking/notebooks/foo_dir\n",
      "\n",
      "[INFO]: mcce_benchmark.scheduling, schedule_job:\n",
      "\tCreated the bash script for crontab.\n",
      "[INFO]: mcce_benchmark.scheduling, create_crontab:\n",
      "\tCrontab text:\n",
      "#Scheduled from bench_launchjob\n",
      "* * * * * /home/cat/projects/MCCE_Benchmarking/notebooks/foo_dir/crontab_default_run_sh > /home/cat/projects/MCCE_Benchmarking/notebooks/foo_dir/cron_default_run.log 2>&1\n",
      "\n",
      "[INFO]: mcce_benchmark.scheduling, create_crontab:\n",
      "\tUser's cron jobs, if any:\n",
      "[INFO]: mcce_benchmark.scheduling, create_crontab:\n",
      "\t# Scheduled from mccebench\n",
      "* * * * * cd /home/cat/projects/mcce_benchmarks && /home/cat/miniconda3/envs/mce/bin/mccebench_launchjob -benchmarks_dir /home/cat/projects/mcce_benchmarks -job_name foo -n_active 4 -sentinel_file step2_out.pdb\n",
      "\n",
      "[INFO]: mcce_benchmark.scheduling, schedule_job:\n",
      "\tScheduled batch submission with crontab every minute.\n",
      "[INFO]: mcce_benchmark.cli, log_mcce_version:\n",
      "\tMCCE Version(s) found in run.log files:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cmd = f\"{cli.SUB_CMD2} -benchmarks_dir {ben_dir}\".split()\n",
    "cmd\n",
    "\n",
    "cli.bench_cli(cmd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Scheduling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO]: mcce_benchmark.cli, <module>:\n",
      "\t\n",
      "START\n",
      "----------------------------------------------------------------------\n",
      "2024-02-28 13:41:41 - USER = 'cat' - User envir: mce\n",
      "APP VER: (0, 1, 0, 'dev5', 'g2095b87.d20240226')\n",
      "APP DEFAULTS:\n",
      "Globals: MCCE_EPS = 4; N_ACTIVE = 10\n",
      "Default resource names:\n",
      "DEFAULT_DIR = 'mcce_benchmarks' : default benchmarking folder name\n",
      "BENCH.CLEAN_PDBS = 'clean_pdbs' : fixed\n",
      "BENCH.Q_BOOK = 'book.txt' : jobs bookkeeping file\n",
      "BENCH.DEFAULT_JOB = 'default_run' (-> default_run.sh script in clean_pdbs/)\n",
      "BENCH.BENCH_PARSE_E4 = PosixPath('/home/cat/projects/MCCE_Benchmarking/mcce_benchmark/data/refsets/parse.e4') : Current reference set\n",
      "N_PDBS = 120 : number of pdbs in the dataset\n",
      "Default analysis output file names (fixed):\n",
      "OUT_FILES.MATCHED_PKAS_FILE.name = 'MATCHED_PKAS_FILE'\n",
      "OUT_FILES.ALL_PKAS_FILE.name = 'ALL_PKAS_FILE'\n",
      "OUT_FILES.CONF_COUNTS.name = 'CONF_COUNTS'\n",
      "OUT_FILES.RES_COUNTS.name = 'RES_COUNTS'\n",
      "OUT_FILES.RUN_TIMES.name = 'RUN_TIMES'\n",
      "OUT_FILES.CONFS_PER_RES.name = 'CONFS_PER_RES'\n",
      "OUT_FILES.CONFS_THRUPUT.name = 'CONFS_THRUPUT'\n",
      "----------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from mcce_benchmark import scheduling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['CONDA_PATH',\n",
       " 'CRON_COMMENT',\n",
       " 'CRON_SH_PREFIX',\n",
       " 'CronTab',\n",
       " 'ENTRY_POINTS',\n",
       " 'Path',\n",
       " 'Pathok',\n",
       " 'USER',\n",
       " 'USER_ENV',\n",
       " 'USER_PRFX',\n",
       " 'Union',\n",
       " 'argNamespace',\n",
       " 'create_cron_sh',\n",
       " 'create_crontab',\n",
       " 'create_crontab_old',\n",
       " 'logger',\n",
       " 'logging',\n",
       " 'make_executable',\n",
       " 'schedule_job',\n",
       " 'shutil',\n",
       " 'subprocess',\n",
       " 'subprocess_run',\n",
       " 'sys']"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fdir(scheduling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('/home/cat/miniconda3/envs/mce', 'mce', '/home/cat/miniconda3/condabin/conda')"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'mccebench_launchjob'"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "PosixPath('/home/cat/projects/mcce_benchmarks_test')"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "USER_PRFX, USER_ENV, CONDA_PATH\n",
    "ENTRY_POINTS[\"launch\"]\n",
    "benchmarks_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "cron_txt = scheduling.create_single_crontab( benchmarks_dir, job, debug=True)\n",
    "print(cron_txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Test cli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "PosixPath('/home/cat/projects/mcce_benchmarks_test/clean_pdbs/norun_foo.sh')"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "#!/bin/bash\n",
      "\n",
      "step1.py prot.pdb --norun\n"
     ]
    }
   ],
   "source": [
    "job = \"norun_foo\"\n",
    "custom_sh.write_run_script_from_template(benchmarks_dir, job,\n",
    "                                                    script_template = ScriptChoices.NORUN,\n",
    "                                                    job_args = None)\n",
    "sh_path = benchmarks_dir.joinpath(BENCH.CLEAN_PDBS, f\"{job}.sh\")\n",
    "sh_path.exists()\n",
    "sh_path\n",
    "!cat {sh_path}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " #!/bin/bash\n",
      "\n",
      " step1.py prot.pdb --dry -d 8.0\n",
      " step2.py -d 8.0\n",
      " step3.py -d 8.0\n",
      " step4.py --xts \n",
      "\n",
      " sleep 10\n"
     ]
    }
   ],
   "source": [
    "!cat {sh_path}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "use_default_script = check_steps_opt_defaults(args)\n",
    "use_default_script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " #!/bin/bash\n",
      "\n",
      " step1.py prot.pdb --dry -d 8.0\n",
      " step2.py -d 8.0\n",
      " step3.py -d 8.0\n",
      " step4.py --xts \n",
      "\n",
      " sleep 10\n",
      "\n"
     ]
    }
   ],
   "source": [
    "out = populate_custom_template(args)\n",
    "print(out)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:mce]",
   "language": "python",
   "name": "conda-env-mce-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
