{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Run the first 2 code cells without modifications_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python ver: 3.11.5 | packaged by conda-forge | (main, Aug 27 2023, 03:34:09) [GCC 12.3.0]\n",
      "Python env: mce\n",
      "Currrent dir: /home/cat/projects/MCCE_Benchmarking/notebooks/_tmp\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "from IPython.display import Markdown #, IFrame\n",
    "# for presentations:\n",
    "#display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "#pd.set_option(\"display.max_colwidth\", 200)\n",
    "import matplotlib as mpl\n",
    "from matplotlib import pyplot as plt\n",
    "plt.ion()\n",
    "plt.style.use('seaborn-v0_8-muted')\n",
    "from pprint import pprint as ptp\n",
    "\n",
    "import sys\n",
    "from pathlib import Path\n",
    "print('Python ver: {}\\nPython env: {}'.format(sys.version, Path(sys.prefix).name))\n",
    "print('Currrent dir: {}\\n'.format(Path.cwd()))\n",
    "\n",
    "\n",
    "def add_to_sys_path(this_path, up=False):\n",
    "    \"\"\"\n",
    "    Prepend this_path to sys.path.\n",
    "    If up=True, path refers to parent folder (1 level up).\n",
    "    \"\"\"\n",
    "\n",
    "    if up:\n",
    "        newp = str(Path(this_path).parent)\n",
    "    else:\n",
    "        newp = str(Path(this_path))\n",
    "    if newp not in sys.path:\n",
    "        sys.path.insert(1, newp)\n",
    "        print('Path added to sys.path: {}'.format(newp))\n",
    "\n",
    "\n",
    "def fdir(obj, start_with_str='_', exclude=True):\n",
    "    \"\"\"Filtered dir() for method discovery.\"\"\"\n",
    "    return [d for d in dir(obj) if not d.startswith(start_with_str) == exclude]\n",
    "\n",
    "def despine(which=['top','right']):\n",
    "    \"\"\"which ([str])): 'left','top','right','bottom'.\"\"\"\n",
    "\n",
    "    ax = plt.gca()\n",
    "    for side in which:\n",
    "        ax.spines[side].set_visible(False)\n",
    "    return\n",
    "\n",
    "def md_width_comment(w:int=120) -> str:\n",
    "    \"\"\"Width guide for composing md documents.\"\"\"\n",
    "    return f\"<!-- dotted line width = {w}\\n{'.'*w}-->\"\n",
    "\n",
    "\n",
    "# autoreload extension\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "notebooks_dir = Path.cwd()\n",
    "add_to_sys_path(notebooks_dir, up=True)\n",
    "notebooks_dir"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "# OUTDATED!\n",
    "\n",
    "```mermaid\n",
    "flowchart TD\n",
    "    exp[Entry point: 'experimental_pkas'\n",
    "    default script: dry prot, 4 steps w/mfe] -->|Input: 'benchmards_dir'| dir{dir exists?}\n",
    "    dir -->|N| ng1[STOP:\n",
    "    Rename existing dir or\n",
    "    Change 'benchmarks_dir']\n",
    "    dir -->|Y| sub1{Subcommand choice:\n",
    "    1. 'benchmark_setup'\n",
    "    2. 'analyze'}\n",
    "    sub1 -->| 'benchmark_setup' |do1[Actions performed:\n",
    "    1. Data setup\n",
    "    2. Scheduling setup\n",
    "    3. Launch]\n",
    "    sub1 -->| 'analyze' | prob1[Problem:\n",
    "    Are there enough completed runs?\n",
    "    Implement with a 'percentage' user input?\n",
    "    => Function needed as initial check\n",
    "    to obtain the completed entries in\n",
    "    the book file & launch the analysis\n",
    "    if % is met.]\n",
    "    prob1 --> runs1{Enough completed runs?}\n",
    "    runs1 -->| Y | rpt1[Final outputs:\n",
    "    * all_pkas file\n",
    "    * Matched pKas file\n",
    "    * Residue stats\n",
    "    * Conformers throughput per step\\n using runtimes & conformer counts\n",
    "    * Plots\n",
    "    * Anything else?]\n",
    "    runs1 --> | N | msg1(Try '>experimental_pkas analyze' later)\n",
    "\n",
    "    mc[Entry point: 'mcce_runs'] -->|Inputs:\n",
    "    2 completed runs:\n",
    "    'new_calc_dir', 'reference_dir';\n",
    "    'titr_type', 'eps'| ref{Which 'reference_dir'?}\n",
    "    ref -->|ref dir is e.g. 'parse.e4' from pKaDB\n",
    "    Applicable only to pH titrations| comp2[Use 'all_pkas.e4' file for comparison]\n",
    "    comp2 --> mcpka[Analysis outputs:\n",
    "    * Matched, then diffed pKa values\n",
    "    * Plot new vs ref for all numeric fields in pK.out]\n",
    "    ref -->|ref dir is another mcce output dir\n",
    "    ASSUMED: runs of same prot| rptmc[Analysis outputs:\n",
    "    * Diffed pK.out\n",
    "    * Residue stats\n",
    "    * Plot new vs ref for all numeric fields in pK.out\n",
    "    * Anything else?]\n",
    "    mcpka -.-> note[Problem:\n",
    "    Analysis will depend on the\n",
    "    contents of 'parse.e4' dir:\n",
    "    full output, partial\n",
    "    or just 'all_pkas.e4' file?]\n",
    "    rptmc -.-> note\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mcce_benchmark import BENCH, DEFAULT_DIR, N_ACTIVE, N_PDBS, ENTRY_POINTS, OUT_FILES, MCCE_OUTPUTS, ANALYZE_DIR\n",
    "from mcce_benchmark import USER_PRFX, USER_ENV, CONDA_PATH\n",
    "from mcce_benchmark import Pathok\n",
    "\n",
    "import subprocess\n",
    "from typing import Union\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "print(BENCH)\n",
    "print(f\"{USER_PRFX = }\\n{USER_ENV = }\\n{CONDA_PATH = }\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/home/cat/projects/MCCE_Benchmarking/notebooks/_tmp')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_dir = PosixPath('/home/cat/projects/mcce_benchmarks_test')\n",
      "benchmarks_dir = PosixPath('/home/cat/projects/mcce_benchmarks_test')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PosixPath('/home/cat/projects/mcce_benchmarks_test/clean_pdbs')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# completed runs:\n",
    "here = Path.cwd()\n",
    "here\n",
    "\n",
    "test_dir = Pathok(Path.cwd().parent.parent.parent.joinpath(\"mcce_benchmarks_test\"))\n",
    "print(f\"{test_dir = }\")\n",
    "\n",
    "benchmarks_dir = Pathok(Path.cwd().parent.parent.parent.joinpath(\"mcce_benchmarks_test\"))\n",
    "print(f\"{benchmarks_dir = }\")\n",
    "\n",
    "pdbs = benchmarks_dir.joinpath(BENCH.CLEAN_PDBS)\n",
    "pdbs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "# Refset prep:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "---\n",
    "\n",
    "# prep download copy of completed refset from isis; keep pK.out until pkanalysis run on folder\n",
    "\n",
    "```\n",
    "total 184\r\n",
    "-rw-r--r--   1 cat cat 173762 Feb 16 10:08 WT_pkas.csv\r\n",
    "-rw-r--r--   1 cat cat    483 Feb 16 10:13 benchmark.log\r\n",
    "drwxr-xr-x 122 cat cat   4096 Feb 16 10:13 clean_pdbs\r\n",
    "-rw-r--r--   1 cat cat   2763 Feb 16 10:08 proteins\n",
    "```\n",
    "# cleanup of mcce_benchmarks copy from isis:\r\n",
    "\r\n",
    "1. get\r\n",
    "scp -r cchenal@134.74.25.124/:~projects/mcce_benchm\n",
    "arks/ .m t files exceptbe  kept:\r\n",
    "  -rucn.prm. rord\r\n",
    "  -  run.lp2_out.pdb\n",
    "  - new.tpl oprot.tpl  ave43 pr\n",
    " h\n",
    "  one)    - pK.out :: until anal\n",
    "ison():.tsv"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "# Use jmao's benchmark folder as testing data\n",
    "0. Rename clean_pdb/ -> clean_pdbs/            [x]\n",
    "1. get list of full runs                       [x]\n",
    "(base) cat@LABTOP:~/projects/benchmark$\n",
    "find clean_pdbs/*/ -type f -name \"pK.out\"\n",
    "\n",
    "2. match dir & copy all files except prot.pdb to mcce_benchmarks_test\n",
    "\n",
    "\n",
    "# To have complete runs in canonical folder structure:\n",
    "\n",
    "# jmao benchamrk folder\n",
    "jmao_dir = Path.cwd().parent.parent.joinpath(\"benchmark\", \"e08_calc\") #-> like clean_pdbs\n",
    "print(f\"{jmao_dir = }\", jmao_dir.exists())\n",
    "\n",
    "#list_complete_runs(jmao_dir, like_clean_pdbs=True)\n",
    "#audit.cp_completed_runs(jmao_dir, test_dir)\n",
    "\n",
    "#audit.list_complete_runs(test_dir)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "cleanup.prep_refset(benchmarks_dir, keep_files=files_retained)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "pkanalysis.collate_all_pkas(test_dir)\n",
    "pkanalysis.all_pkas_df(test_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import logging\n",
    "logger = logging.getLogger('cli')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Test cli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "from argparse import ArgumentParser, ArgumentError, RawDescriptionHelpFormatter, Namespace as argNamespace\n",
    "from mcce_benchmark import cli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "clilogr = cli.logger\n",
    "clilogr.hasHandlers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: \n",
      "bench_expl_pkas <+ 1 sub-command: setup_job or launch_job> <related args>\n",
      "\n",
      "Examples:\n",
      "1. setup_job: Data & script setup:\n",
      "   - Minimal input: value for -benchmarks_dir option:\n",
      "     >bench_expl_pkas setup_job -benchmarks_dir <folder name>\n",
      "\n",
      "   - Using non-default option(s):\n",
      "     >bench_expl_pkas setup_job -benchmarks_dir <folder name> -d 8\n",
      "\n",
      "2. launch_job: Launch runs:\n",
      "   - Minimal input: value for -benchmarks_dir option:\n",
      "     >bench_expl_pkas launch_job -benchmarks_dir <folder name>\n",
      "\n",
      "   - Using non-default option(s):\n",
      "     >bench_expl_pkas launch_job -benchmarks_dir <folder name> -n_active <jobs to maintain>\n",
      "     >bench_expl_pkas launch_job -benchmarks_dir <folder name> -job_name <my_job_name> -sentinel_file step2_out.pdb\n",
      "\n",
      "Description:\n",
      "Launch a MCCE benchmarking job using curated structures from the pKa Database v1.\n",
      "\n",
      "The main command is bench_expl_pkas along with one of 2 sub-commands:\n",
      "- Sub-command 1: setup_job: setup the dataset and run script to run mcce steps 1 through 4;\n",
      "- Sub-command 2: launch_job: launch a batch of jobs;\n",
      "\n",
      "options:\n",
      "  -h, --help            show this help message and exit\n",
      "\n",
      "bench_expl_pkas sub-commands:\n",
      "  Sub-commands of MCCE benchmarking cli.\n",
      "\n",
      "  {setup_job,launch_job}\n",
      "                        The 3 choices for the benchmarking process: 1) Setup\n",
      "                        data & run-script: {SUB_CMD1} 2) Batch-run mcce steps\n",
      "                        1 through 4: {SUB_CMD2}\n",
      "    setup_job           Sub-command for setting up <benchmarks_dir>/clean_pdbs\n",
      "                        folder & job_name_run.sh script, e.g.:\n",
      "                        >bench_expl_pkas setup_job -benchmarks_dir <folder\n",
      "                        name>\n",
      "    launch_job          Sub-command for launching a batch of jobs, e.g.:\n",
      "                        >bench_expl_pkas launch_job -benchmarks_dir <folder\n",
      "                        name> -n_active 15 Note: if provided, the value for\n",
      "                        the -job_name option must match the one used in\n",
      "                        `setup_job`.\n",
      "\n",
      "Post an issue for all errors and feature requests at:\n",
      "https://github.com/GunnerLab/MCCE_Benchmarking/issues\n"
     ]
    }
   ],
   "source": [
    "cli_parser = cli.bench_parser()\n",
    "\n",
    "cli_parser.print_help()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Test help msg\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## cli: None input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Description:\n",
      "Launch a MCCE benchmarking job using curated structures from the pKa Database v1.\n",
      "\n",
      "The main command is bench_expl_pkas along with one of 2 sub-commands:\n",
      "- Sub-command 1: setup_job: setup the dataset and run script to run mcce steps 1 through 4;\n",
      "- Sub-command 2: launch_job: launch a batch of jobs;\n",
      "\n",
      "\n",
      "bench_expl_pkas <+ 1 sub-command: setup_job or launch_job> <related args>\n",
      "\n",
      "Examples:\n",
      "1. setup_job: Data & script setup:\n",
      "   - Minimal input: value for -benchmarks_dir option:\n",
      "     >bench_expl_pkas setup_job -benchmarks_dir <folder name>\n",
      "\n",
      "   - Using non-default option(s):\n",
      "     >bench_expl_pkas setup_job -benchmarks_dir <folder name> -d 8\n",
      "\n",
      "2. launch_job: Launch runs:\n",
      "   - Minimal input: value for -benchmarks_dir option:\n",
      "     >bench_expl_pkas launch_job -benchmarks_dir <folder name>\n",
      "\n",
      "   - Using non-default option(s):\n",
      "     >bench_expl_pkas launch_job -benchmarks_dir <folder name> -n_active <jobs to maintain>\n",
      "     >bench_expl_pkas launch_job -benchmarks_dir <folder name> -job_name <my_job_name> -sentinel_file step2_out.pdb\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cmd = None\n",
    "cli.bench_cli(cmd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Description:\n",
      "Launch a MCCE benchmarking job using curated structures from the pKa Database v1.\n",
      "\n",
      "The main command is bench_expl_pkas along with one of 2 sub-commands:\n",
      "- Sub-command 1: setup_job: setup the dataset and run script to run mcce steps 1 through 4;\n",
      "- Sub-command 2: launch_job: launch a batch of jobs;\n",
      "\n",
      "\n",
      "bench_expl_pkas <+ 1 sub-command: setup_job or launch_job> <related args>\n",
      "\n",
      "Examples:\n",
      "1. setup_job: Data & script setup:\n",
      "   - Minimal input: value for -benchmarks_dir option:\n",
      "     >bench_expl_pkas setup_job -benchmarks_dir <folder name>\n",
      "\n",
      "   - Using non-default option(s):\n",
      "     >bench_expl_pkas setup_job -benchmarks_dir <folder name> -d 8\n",
      "\n",
      "2. launch_job: Launch runs:\n",
      "   - Minimal input: value for -benchmarks_dir option:\n",
      "     >bench_expl_pkas launch_job -benchmarks_dir <folder name>\n",
      "\n",
      "   - Using non-default option(s):\n",
      "     >bench_expl_pkas launch_job -benchmarks_dir <folder name> -n_active <jobs to maintain>\n",
      "     >bench_expl_pkas launch_job -benchmarks_dir <folder name> -job_name <my_job_name> -sentinel_file step2_out.pdb\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cmd = []\n",
    "cli.bench_cli(cmd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## cli: 1 input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['setup_job']"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sub-command for setting up <benchmarks_dir>/clean_pdbs folder & job_name_run.sh script, e.g.:\n",
      ">bench_expl_pkas setup_job -benchmarks_dir <folder name>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cmd = f\"{cli.SUB_CMD1}\".split()\n",
    "cmd\n",
    "cli.bench_cli(cmd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sub-command for launching a batch of jobs, e.g.:\n",
      ">bench_expl_pkas launch_job -benchmarks_dir <folder name> -n_active 15\n",
      "Note: if provided, the value for the -job_name option must match the one used in `setup_job`.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cmd = f\"{cli.SUB_CMD2}\".split() #benchmarks_dir {benchmarks_dir} -job_name {job} -d 8\".split()\n",
    "\n",
    "cli.bench_cli(cmd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## cli: 2 with help"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['setup_job', '-h']"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Description:\n",
      "Launch a MCCE benchmarking job using curated structures from the pKa Database v1.\n",
      "\n",
      "The main command is bench_expl_pkas along with one of 2 sub-commands:\n",
      "- Sub-command 1: setup_job: setup the dataset and run script to run mcce steps 1 through 4;\n",
      "- Sub-command 2: launch_job: launch a batch of jobs;\n",
      "\n",
      "\n",
      "bench_expl_pkas <+ 1 sub-command: setup_job or launch_job> <related args>\n",
      "\n",
      "Examples:\n",
      "1. setup_job: Data & script setup:\n",
      "   - Minimal input: value for -benchmarks_dir option:\n",
      "     >bench_expl_pkas setup_job -benchmarks_dir <folder name>\n",
      "\n",
      "   - Using non-default option(s):\n",
      "     >bench_expl_pkas setup_job -benchmarks_dir <folder name> -d 8\n",
      "\n",
      "2. launch_job: Launch runs:\n",
      "   - Minimal input: value for -benchmarks_dir option:\n",
      "     >bench_expl_pkas launch_job -benchmarks_dir <folder name>\n",
      "\n",
      "   - Using non-default option(s):\n",
      "     >bench_expl_pkas launch_job -benchmarks_dir <folder name> -n_active <jobs to maintain>\n",
      "     >bench_expl_pkas launch_job -benchmarks_dir <folder name> -job_name <my_job_name> -sentinel_file step2_out.pdb\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cmd = f\"{cli.SUB_CMD1} -h\".split() #benchmarks_dir {benchmarks_dir} -job_name {job} -d 8\".split()\n",
    "cmd\n",
    "#args = cli_parser.parse_args(cmd)\n",
    "\n",
    "cli.bench_cli(cmd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## cli: setup with n_pdbs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mcce_benchmark import custom_sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO]: mcce_benchmark.cli, <module>:\n",
      "\t\n",
      "START\n",
      "----------------------------------------------------------------------\n",
      "2024-02-28 13:41:41 - USER = 'cat' - User envir: mce\n",
      "APP VER: (0, 1, 0, 'dev5', 'g2095b87.d20240226')\n",
      "APP DEFAULTS:\n",
      "Globals: MCCE_EPS = 4; N_ACTIVE = 10\n",
      "Default resource names:\n",
      "DEFAULT_DIR = 'mcce_benchmarks' : default benchmarking folder name\n",
      "BENCH.CLEAN_PDBS = 'clean_pdbs' : fixed\n",
      "BENCH.Q_BOOK = 'book.txt' : jobs bookkeeping file\n",
      "BENCH.DEFAULT_JOB = 'default_run' (-> default_run.sh script in clean_pdbs/)\n",
      "BENCH.BENCH_PARSE_E4 = PosixPath('/home/cat/projects/MCCE_Benchmarking/mcce_benchmark/data/refsets/parse.e4') : Current reference set\n",
      "N_PDBS = 120 : number of pdbs in the dataset\n",
      "Default analysis output file names (fixed):\n",
      "OUT_FILES.MATCHED_PKAS_FILE.name = 'MATCHED_PKAS_FILE'\n",
      "OUT_FILES.ALL_PKAS_FILE.name = 'ALL_PKAS_FILE'\n",
      "OUT_FILES.CONF_COUNTS.name = 'CONF_COUNTS'\n",
      "OUT_FILES.RES_COUNTS.name = 'RES_COUNTS'\n",
      "OUT_FILES.RUN_TIMES.name = 'RUN_TIMES'\n",
      "OUT_FILES.CONFS_PER_RES.name = 'CONFS_PER_RES'\n",
      "OUT_FILES.CONFS_THRUPUT.name = 'CONFS_THRUPUT'\n",
      "----------------------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['setup_job', '-benchmarks_dir', '../foo_dir', '-n_pdbs', '2']"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ben_dir = \"../foo_dir\"\n",
    "cmd = f\"{cli.SUB_CMD1} -benchmarks_dir {ben_dir} -n_pdbs 2\".split() # -job_name {job} -d 8\".split()\n",
    "cmd\n",
    "args = cli_parser.parse_args(cmd)\n",
    "\n",
    "all_default = custom_sh.all_opts_are_defaults(args)\n",
    "all_default\n",
    "\n",
    "#cli.bench_cli(cmd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['launch_job', '-benchmarks_dir', '../foo_dir']"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO]: mcce_benchmark.cli, bench_launch_batch:\n",
      "\tbench_expl_pkas args:\n",
      "{'subparser_name': 'launch_job',\n",
      " 'benchmarks_dir': PosixPath('/home/cat/projects/MCCE_Benchmarking/notebooks/foo_dir'),\n",
      " 'job_name': 'default_run',\n",
      " 'n_active': 10,\n",
      " 'sentinel_file': 'pK.out',\n",
      " 'func': <function mcce_benchmark.cli.bench_launch_batch(args: argparse.Namespace) -> None>}\n",
      "\n",
      "[INFO]: mcce_benchmark.cli, bench_launch_batch:\n",
      "\tScript contents prior to launch:\n",
      "#!/bin/bash\n",
      "\n",
      "step1.py --dry prot.pdb\n",
      "step2.py -d 4\n",
      "step3.py -d 4\n",
      "step4.py --xts\n",
      "\n",
      "sleep 10\n",
      "\n",
      "[INFO]: mcce_benchmark.cli, bench_launch_batch:\n",
      "\tSubmiting batch of jobs.\n",
      "[INFO]: mcce_benchmark.scheduling, create_cron_sh:\n",
      "\tCreated script for crontab 'crontab_default_run_sh' in /home/cat/projects/MCCE_Benchmarking/notebooks/foo_dir\n",
      "\n",
      "[INFO]: mcce_benchmark.scheduling, schedule_job:\n",
      "\tCreated the bash script for crontab.\n",
      "[INFO]: mcce_benchmark.scheduling, create_crontab:\n",
      "\tCrontab text:\n",
      "#Scheduled from bench_launchjob\n",
      "* * * * * /home/cat/projects/MCCE_Benchmarking/notebooks/foo_dir/crontab_default_run_sh > /home/cat/projects/MCCE_Benchmarking/notebooks/foo_dir/cron_default_run.log 2>&1\n",
      "\n",
      "[INFO]: mcce_benchmark.scheduling, create_crontab:\n",
      "\tUser's cron jobs, if any:\n",
      "[INFO]: mcce_benchmark.scheduling, create_crontab:\n",
      "\t# Scheduled from mccebench\n",
      "* * * * * cd /home/cat/projects/mcce_benchmarks && /home/cat/miniconda3/envs/mce/bin/mccebench_launchjob -benchmarks_dir /home/cat/projects/mcce_benchmarks -job_name foo -n_active 4 -sentinel_file step2_out.pdb\n",
      "\n",
      "[INFO]: mcce_benchmark.scheduling, schedule_job:\n",
      "\tScheduled batch submission with crontab every minute.\n",
      "[INFO]: mcce_benchmark.cli, log_mcce_version:\n",
      "\tMCCE Version(s) found in run.log files:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cmd = f\"{cli.SUB_CMD2} -benchmarks_dir {ben_dir}\".split()\n",
    "cmd\n",
    "\n",
    "cli.bench_cli(cmd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Scheduling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO]: mcce_benchmark.cli, <module>:\n",
      "\t\n",
      "START\n",
      "----------------------------------------------------------------------\n",
      "2024-02-28 13:41:41 - USER = 'cat' - User envir: mce\n",
      "APP VER: (0, 1, 0, 'dev5', 'g2095b87.d20240226')\n",
      "APP DEFAULTS:\n",
      "Globals: MCCE_EPS = 4; N_ACTIVE = 10\n",
      "Default resource names:\n",
      "DEFAULT_DIR = 'mcce_benchmarks' : default benchmarking folder name\n",
      "BENCH.CLEAN_PDBS = 'clean_pdbs' : fixed\n",
      "BENCH.Q_BOOK = 'book.txt' : jobs bookkeeping file\n",
      "BENCH.DEFAULT_JOB = 'default_run' (-> default_run.sh script in clean_pdbs/)\n",
      "BENCH.BENCH_PARSE_E4 = PosixPath('/home/cat/projects/MCCE_Benchmarking/mcce_benchmark/data/refsets/parse.e4') : Current reference set\n",
      "N_PDBS = 120 : number of pdbs in the dataset\n",
      "Default analysis output file names (fixed):\n",
      "OUT_FILES.MATCHED_PKAS_FILE.name = 'MATCHED_PKAS_FILE'\n",
      "OUT_FILES.ALL_PKAS_FILE.name = 'ALL_PKAS_FILE'\n",
      "OUT_FILES.CONF_COUNTS.name = 'CONF_COUNTS'\n",
      "OUT_FILES.RES_COUNTS.name = 'RES_COUNTS'\n",
      "OUT_FILES.RUN_TIMES.name = 'RUN_TIMES'\n",
      "OUT_FILES.CONFS_PER_RES.name = 'CONFS_PER_RES'\n",
      "OUT_FILES.CONFS_THRUPUT.name = 'CONFS_THRUPUT'\n",
      "----------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from mcce_benchmark import scheduling\n",
    "from crontab import CronTab\n",
    "#from mcce_benchmark.scheduling import subprocess_run\n",
    "#import subprocess_run, build_cron_cmd, create_crontab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Scheduled from bench_launchjob\n",
      "* * * * * /home/cat/projects/MCCE_Benchmarking/notebooks/foo_dir/crontab_default_run_sh > /home/cat/projects/MCCE_Benchmarking/notebooks/foo_dir/cron_default_run.log 2>&1\n"
     ]
    }
   ],
   "source": [
    "cron = CronTab(user=True)\n",
    "if cron.crons:\n",
    "    for ct in cron:\n",
    "        print(ct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['CONDA_PATH',\n",
       " 'CRON_COMMENT',\n",
       " 'CRON_SH_PREFIX',\n",
       " 'CronTab',\n",
       " 'ENTRY_POINTS',\n",
       " 'Path',\n",
       " 'Pathok',\n",
       " 'USER',\n",
       " 'USER_ENV',\n",
       " 'USER_PRFX',\n",
       " 'Union',\n",
       " 'argNamespace',\n",
       " 'create_cron_sh',\n",
       " 'create_crontab',\n",
       " 'create_crontab_old',\n",
       " 'logger',\n",
       " 'logging',\n",
       " 'make_executable',\n",
       " 'schedule_job',\n",
       " 'shutil',\n",
       " 'subprocess',\n",
       " 'subprocess_run',\n",
       " 'sys']"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fdir(scheduling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('/home/cat/miniconda3/envs/mce', 'mce', '/home/cat/miniconda3/condabin/conda')"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'mccebench_launchjob'"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "PosixPath('/home/cat/projects/mcce_benchmarks_test')"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "USER_PRFX, USER_ENV, CONDA_PATH\n",
    "ENTRY_POINTS[\"launch\"]\n",
    "benchmarks_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO]: mcce_benchmark.scheduling, create_cron_sh:\n",
      "\tCreated script for crontab 'crontab_baz_sh' in /home/cat/projects/mcce_benchmarks_test\n",
      "\n",
      "[INFO]: mcce_benchmark.scheduling, create_cron_sh:\n",
      "\tCreated script for crontab 'crontab_baz_sh' in /home/cat/projects/mcce_benchmarks_test\n",
      "\n",
      "[INFO]: mcce_benchmark.scheduling, create_cron_sh:\n",
      "\tCreated script for crontab 'crontab_baz_sh' in /home/cat/projects/mcce_benchmarks_test\n",
      "\n",
      "[INFO]: mcce_benchmark.scheduling, create_cron_sh:\n",
      "\tCreated script for crontab 'crontab_baz_sh' in /home/cat/projects/mcce_benchmarks_test\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO]: mcce_benchmark.scheduling, create_cron_sh:\n",
      "\tCreated script for crontab 'crontab_baz_sh' in /home/cat/projects/mcce_benchmarks_test\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PosixPath('/home/cat/projects/mcce_benchmarks_test/crontab_baz_sh')"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "job = \"baz\"\n",
    "conda_sh = scheduling.create_cron_sh(benchmarks_dir, job, 2, \"pK.out\")\n",
    "conda_sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pgrep -u cat foo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "#!/usr/bin/env sh\n",
      "\n",
      "/home/cat/miniconda3/condabin/conda run -n mce mccebench_launchjob -benchmarks_dir /home/cat/projects/mcce_benchmarks_test -job_name baz -n_active 2 -sentinel_file pK.out\n"
     ]
    }
   ],
   "source": [
    "!cat {conda_sh}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO]: mcce_benchmark.scheduling, create_crontab:\n",
      "\tCrontab text:\n",
      "#Scheduled from mccebench_launchjob\n",
      "* * * * * /home/cat/projects/mcce_benchmarks_test/crontab_baz_sh > /home/cat/projects/mcce_benchmarks_test/cron_baz.log 2>&1\n",
      "\n",
      "[INFO]: mcce_benchmark.scheduling, create_crontab:\n",
      "\tCrontab text:\n",
      "#Scheduled from mccebench_launchjob\n",
      "* * * * * /home/cat/projects/mcce_benchmarks_test/crontab_baz_sh > /home/cat/projects/mcce_benchmarks_test/cron_baz.log 2>&1\n",
      "\n",
      "[INFO]: mcce_benchmark.scheduling, create_crontab:\n",
      "\tCrontab text:\n",
      "#Scheduled from mccebench_launchjob\n",
      "* * * * * /home/cat/projects/mcce_benchmarks_test/crontab_baz_sh > /home/cat/projects/mcce_benchmarks_test/cron_baz.log 2>&1\n",
      "\n",
      "[INFO]: mcce_benchmark.scheduling, create_crontab:\n",
      "\tCrontab text:\n",
      "#Scheduled from mccebench_launchjob\n",
      "* * * * * /home/cat/projects/mcce_benchmarks_test/crontab_baz_sh > /home/cat/projects/mcce_benchmarks_test/cron_baz.log 2>&1\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO]: mcce_benchmark.scheduling, create_crontab:\n",
      "\tCrontab text:\n",
      "#Scheduled from mccebench_launchjob\n",
      "* * * * * /home/cat/projects/mcce_benchmarks_test/crontab_baz_sh > /home/cat/projects/mcce_benchmarks_test/cron_baz.log 2>&1\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO]: mcce_benchmark.scheduling, create_crontab:\n",
      "\tUser's cron jobs, if any:\n",
      "[INFO]: mcce_benchmark.scheduling, create_crontab:\n",
      "\tUser's cron jobs, if any:\n",
      "[INFO]: mcce_benchmark.scheduling, create_crontab:\n",
      "\tUser's cron jobs, if any:\n",
      "[INFO]: mcce_benchmark.scheduling, create_crontab:\n",
      "\tUser's cron jobs, if any:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO]: mcce_benchmark.scheduling, create_crontab:\n",
      "\tUser's cron jobs, if any:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO]: mcce_benchmark.scheduling, create_crontab:\n",
      "\t# Scheduled from mccebench\n",
      "* * * * * cd /home/cat/projects/mcce_benchmarks && /home/cat/miniconda3/envs/mce/bin/mccebench_launchjob -benchmarks_dir /home/cat/projects/mcce_benchmarks -job_name foo -n_active 4 -sentinel_file step2_out.pdb\n",
      "\n",
      "[INFO]: mcce_benchmark.scheduling, create_crontab:\n",
      "\t# Scheduled from mccebench\n",
      "* * * * * cd /home/cat/projects/mcce_benchmarks && /home/cat/miniconda3/envs/mce/bin/mccebench_launchjob -benchmarks_dir /home/cat/projects/mcce_benchmarks -job_name foo -n_active 4 -sentinel_file step2_out.pdb\n",
      "\n",
      "[INFO]: mcce_benchmark.scheduling, create_crontab:\n",
      "\t# Scheduled from mccebench\n",
      "* * * * * cd /home/cat/projects/mcce_benchmarks && /home/cat/miniconda3/envs/mce/bin/mccebench_launchjob -benchmarks_dir /home/cat/projects/mcce_benchmarks -job_name foo -n_active 4 -sentinel_file step2_out.pdb\n",
      "\n",
      "[INFO]: mcce_benchmark.scheduling, create_crontab:\n",
      "\t# Scheduled from mccebench\n",
      "* * * * * cd /home/cat/projects/mcce_benchmarks && /home/cat/miniconda3/envs/mce/bin/mccebench_launchjob -benchmarks_dir /home/cat/projects/mcce_benchmarks -job_name foo -n_active 4 -sentinel_file step2_out.pdb\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO]: mcce_benchmark.scheduling, create_crontab:\n",
      "\t# Scheduled from mccebench\n",
      "* * * * * cd /home/cat/projects/mcce_benchmarks && /home/cat/miniconda3/envs/mce/bin/mccebench_launchjob -benchmarks_dir /home/cat/projects/mcce_benchmarks -job_name foo -n_active 4 -sentinel_file step2_out.pdb\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#Scheduled from mccebench_launchjob\n",
      "* * * * * /home/cat/projects/mcce_benchmarks_test/crontab_baz_sh > /home/cat/projects/mcce_benchmarks_test/cron_baz.log 2>&1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cron_txt = scheduling.create_crontab(conda_sh, benchmarks_dir, job, debug=True)\n",
    "print(cron_txt)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "(base) cat@LABTOP:~/projects$ which mccebench\n",
    "/home/cat/miniconda3/envs/mce/bin/mccebench\n",
    "\n",
    "(base) cat@LABTOP:~/projects$ python -c \"import sys; print(sys.base_prefix, sys.prefix)\"\n",
    "/home/cat/miniconda3 /home/cat/miniconda3\n",
    "\n",
    "(mce) cat@LABTOP:~/projects$ python -c \"import sys; print(sys.base_prefix, sys.prefix)\"\n",
    "/home/cat/miniconda3/envs/mce /home/cat/miniconda3/envs/mce\n",
    "/home/cat/miniconda3/envs/mce/bin/mccebench\n",
    "\n",
    "(base) cat@LABTOP:~$ cat .conda/environments.txt\n",
    "/home/cat/miniconda3\n",
    "/home/cat/miniconda3/envs/mce"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "On isis:\n",
    "(base) cchenal@isis:~/projects$ which step1.py\n",
    "/home/mcce/Stable-MCCE/bin/step1.py"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# from jmao:\n",
    "PATH=/home/jmao/miniconda3/bin:/home/jmao/Stable-MCCE/bin:/home/jmao/bin:/home/jmao/miniconda3/condabin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin\n",
    "*  *   *   *   *     cd /home/jmao/benchmark/e08_calc && /home/jmao/benchmark/bin/batch_submit.py >/tmp/cron.log 2>&1\n",
    "\n",
    "#..................................................\n",
    "import subprocess\n",
    "\n",
    "cron_in = subprocess.Popen(['crontab', '-l'], stdout=subprocess.PIPE)\n",
    "cur_crontab, _ = cron_in.communicate()\n",
    "# new_crontab = do_my_magic(cur_crontab)\n",
    "cron_out = subprocess.Popen(['crontab', '-'], stdin=subprocess.PIPE)\n",
    "cron_out.communicate(input=new_crontab)\n",
    "\n",
    "#..................................................\n",
    "#https://stackoverflow.com/questions/878600/how-to-create-a-cron-job-using-bash-automatically-without-the-interactive-editor\n",
    "\n",
    "cat > /etc/cron.d/<job> << EOF\n",
    "SHELL=/bin/bash \n",
    "PATH=/sbin:/bin:/usr/sbin:/usr/bin \n",
    "01 * * * * <user> <command>\n",
    "EOF\n",
    "\n",
    "#.................................................\n",
    "This would insert the cron jobs listed in my-cron-jobs.txt into the crontab, while avoiding creating duplicate entries:\n",
    "\n",
    "((crontab -l 2>/dev/null || true; echo \"1 * * * * echo Hello\") | sort -u) | crontab -\n",
    "\n",
    "#..................................................\n",
    "TEST: permision denied for either way:\n",
    "\n",
    "sudo echo \"1 * * * * echo Hello\" > /etc/cron.d/test_echo\n",
    "\n",
    "cat > /etc/cron.d/test_echo << EOF\n",
    "SHELL=/bin/bash\n",
    "PATH=/sbin:/bin:/usr/sbin:/usr/bin\n",
    "1 * * * * cat echo Hello!\n",
    "EOF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#Scheduled from mccebench\n",
      "PATH=/home/cat/miniconda3/bin:/home/cat/miniconda3/envs/mce/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin\n",
      "\n",
      "#Scheduled from mccebench\n",
      "* * * * * cd /home/cat/projects/mcce_benchmarks && /home/cat/miniconda3/envs/mce/bin/mccebench_launchjob -benchmarks_dir /home/cat/projects/mcce_benchmarks -job_name foo -n_active 4 -sentinel_file step2_out.pdb\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO]: mcce_benchmark.scheduling, create_crontab:\n",
      "\tCrontab text:\n",
      "#Scheduled from mccebench\n",
      "PATH=/home/cat/miniconda3/bin:/home/cat/miniconda3/envs/mce/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin\n",
      "#Scheduled from mccebench\n",
      "* * * * * cd /home/cat/projects/mcce_benchmarks && /home/cat/miniconda3/envs/mce/bin/mccebench_launchjob -benchmarks_dir /home/cat/projects/mcce_benchmarks -job_name foo -n_active 4 -sentinel_file step2_out.pdb\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "no crontab for cat\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO]: mcce_benchmark.scheduling, create_crontab:\n",
      "\tUser's cron jobs, if any:\n"
     ]
    }
   ],
   "source": [
    "PA = build_cron_path()\n",
    "print(PA)\n",
    "launch_cmd = build_cron_cmd(benchmarks_dir, \"foo\", 4,\"step2_out.pdb\")\n",
    "print(launch_cmd)\n",
    "\n",
    "\n",
    "cron = CronTab(user=True)\n",
    "for job in cron:\n",
    "    print(job)\n",
    "\n",
    "cron.remove_all(comment=CRON_COMMENT)\n",
    "\n",
    "create_crontab(PA, launch_cmd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for job in cron:\n",
    "    print(job)\n",
    "    # Iterate over all lines:\n",
    "for line in cron.lines:\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Future Job Setup?"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "mcce_step_options = {\n",
    "    \"S1\":{\"msg\":\"Run mcce step 1, premcce to format PDB file to MCCE PDB format.\",\n",
    "          \"--noter\": {\"default\":False, \"help\":\"Do not label terminal residues (for making ftpl).\", \"action\":\"store_true\"},\n",
    "          \"--dry\":   {\"default\":False, \"help\":\"Delete all water molecules.\", \"action\":\"store_true\"},\n",
    "          },\n",
    "    \"S2\":{\"msg\":\"Run mcce step 2, make side chain conformers from step1_out.pdb.\",\n",
    "          \"-l\":      {\"metavar\":\"level\",\n",
    "                      \"type\":int, \"default\":1,\n",
    "                      \"help\":\"Conformer level 1=quick (default), 2=medium, 3=full\"},\n",
    "          },\n",
    "    \"S3\":{\"msg\":\"Run mcce step 3, energy calculations, with multiple threads.\",\n",
    "          # should have been --r:\n",
    "          \"-r\":      {\"default\":False, \"help\":\"refresh opp files and head3.lst without running delphi\", \"action\":\"store_true\"},\n",
    "          \"-c\":      {\"metavar\":\"('conf start', 'conf end')\",\n",
    "                      \"type\":int,\n",
    "                      \"default\":[1, 99999], \"nargs\":2,\n",
    "                       \"help\":\"starting and ending conformer, default to 1 and 9999\"},\n",
    "          \"-f\":      {\"metavar\":\"tmp folder\", \"default\":\"/tmp\", \"hel\":\"delphi temporary folder, default to /tmp\"},\n",
    "          \"-p\":      {\"metavar\":\"processes\", \"type\":int, \"default\":1,\n",
    "                      \"help\":\"run mcce with p number of processes; default: %(default)s.\"},\n",
    "          },\n",
    "    \"S4\":{\"msg\":\"Run mcce step 4, Monte Carlo sampling to simulate a titration.\",\n",
    "          \"--xts\":   {\"default\":False, \"help\":\"Enable entropy correction, default is false\", \"action\":\"store_true\"},\n",
    "          \"--ms\":    {\"default\":False, \"help\":\"Enable microstate output\", \"action\":\"store_true\"},\n",
    "          \"-t\":      {\"metavar\":\"ph or eh\", \"default\":\"ph\", \"help\":\"titration type: pH or Eh.\"},\n",
    "          \"-i\":      {\"metavar\":\"initial ph/eh\", \"default\":\"0.0\", \"help\":\"Initial pH/Eh of titration; default: %(default)s.\"},\n",
    "          \"-d\":      {\"metavar\":\"interval\", \"default\":\"1.0\", \"help\":\"titration interval in pJ or mV; default: %(default)s.\"},\n",
    "          \"-n\":      {\"metavar\":\"steps\", \"default\":\"15\", \"help\":\"number of steps of titration; default: %(default)s.\"},\n",
    "          }\n",
    "}\n",
    "\n",
    "\n",
    "CLI_NAME = \"mcce_bench\"  # as per pyproject.toml\n",
    "SUB_CMD1, SUB_CMD2 = \"from_step1\", \"from_step3\"\n",
    "USAGE = f\"{CLI_NAME} <sub-command for simulation start> <related args>\\n\"\n",
    "\n",
    "DESC = f\"\"\"\n",
    "    Launch a MCCE benchmarking job using curated structures from the pKa Database v1.\n",
    "\n",
    "    The main command is {CLI_NAME!r} along with one of two sub-commands,\n",
    "    which distinguishes the starting point for the MCCE simulation.\n",
    "    - Sub-command {SUB_CMD1!r}: starts from step1 -> step4;\n",
    "    - Sub-command {SUB_CMD2!r}: starts from step3 -> step4 :: NOT YET IMPLEMENTED!\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "HELP_1 = f\"Sub-command {SUB_CMD1!r} for starting the MCCE simulation from step1.\"\n",
    "HELP_2 = f\"Sub-command {SUB_CMD2!r} for starting the MCCE simulation from step3.\"\n",
    "\n",
    "def bench_from_step1(args):\n",
    "    \"\"\"Benchmark setup and launch for 'from_step1' sub-command.\"\"\"\n",
    "    # TODO\n",
    "    # setup folders\n",
    "    # write <job_name>.sh\n",
    "    # launch\n",
    "    pass\n",
    "\n",
    "\n",
    "def bench_from_step3(args):\n",
    "    \"\"\"Benchmark setup and launch for 'from_step3' sub-command.\"\"\"\n",
    "    # TODO later\n",
    "    pass\n",
    "\n",
    "\n",
    "def bench_parser():\n",
    "    \"\"\"Command line arguments parser with sub-commands for use in benchmarking.\n",
    "    \"\"\"\n",
    "\n",
    "    def arg_valid_dirpath(p: str):\n",
    "        \"\"\"Return resolved path from the command line.\"\"\"\n",
    "        if not len(p):\n",
    "            return None\n",
    "        return Path(p).resolve()\n",
    "\n",
    "    p = ArgumentParser(\n",
    "        prog = f\"{CLI_NAME} \",\n",
    "        description = DESC,\n",
    "        usage = USAGE,\n",
    "        formatter_class = RawDescriptionHelpFormatter,\n",
    "        epilog = \">>> END of %(prog)s.\",\n",
    "    )\n",
    "    subparsers = p.add_subparsers(required=True,\n",
    "                                  title='pipeline step commands',\n",
    "                                  description='Subcommands of the MCCE-CDC processing pipeline',\n",
    "                                  help='The 3 steps of the MCCE-CDC processing pipeline',\n",
    "                                  dest='subparser_name'\n",
    "                                 )\n",
    "\n",
    "    # do_ms_to_pdbs\n",
    "    sub1 = subparsers.add_parser(SUB_CMD1,\n",
    "                                 formatter_class = RawDescriptionHelpFormatter,\n",
    "                                  help=HELP_1)\n",
    "    sub1.add_argument(\n",
    "        \"benchmark_dir\",\n",
    "        type = arg_valid_dirpath,\n",
    "        help = \"\"\"The user's choice of directory for setting up the benchmarking job(s); required.\n",
    "        If the directory does not exists in the location where this cli is called, then it is\n",
    "        created. Recommended name: \"mcce_benchmarks\"; this is where all subsequent jobs will\n",
    "        reside as subfolders.\n",
    "        \"\"\"\n",
    "    )\n",
    "    sub1.add_argument(\n",
    "        \"job_name\",\n",
    "        type = str,\n",
    "        help = \"\"\"The descriptive name, devoid of spaces, for the current job (don't make it too long!); required.\n",
    "        This job_name is be used to name the curent job in 'benchmark_dir' and name the script that launches the\n",
    "        MCCE simulation in ./clean_pdbs folder.\n",
    "        \"\"\"\n",
    "    )\n",
    "    # always 'prot.pdb' as per soft-link setup: ln -s DIR/dir.pdb prot.pdb\n",
    "    #sub1.add_argument(\n",
    "    #    \"-prot\",\n",
    "    #    metavar = \"pdb\",\n",
    "    #    default = \"prot.pdb\",\n",
    "    #    help = \"The name of the pdb; default: %(default)s.\",\n",
    "    )\n",
    "    sub1.add_argument(\n",
    "        \"--dry\",\n",
    "        default = False,\n",
    "        help = \"No water molecules.\",\n",
    "        action = \"store_true\"\n",
    "    )\n",
    "    sub1.add_argument(\n",
    "        \"--norun\",\n",
    "        default = False,\n",
    "        action = \"store_true\",\n",
    "        help = \"Create run.prm without running the step\"\n",
    "    )\n",
    "    sub1.add_argument(\n",
    "        \"-e\",\n",
    "        metavar = \"/path/to/mcce\",\n",
    "        default = \"mcce\",\n",
    "        help = \"Location of the mcce executable, i.e. which mcce; default: %(default)s.\",\n",
    "    )\n",
    "    sub1.add_argument(\n",
    "        \"-eps\",\n",
    "        metavar = \"epsilon\",\n",
    "        default = \"4.0\",\n",
    "        help = \"Protein dielectric constant; default: %(default)s.\",\n",
    "    )\n",
    "    sub1.add_argument(\n",
    "        \"-u\",\n",
    "        metavar = \"Comma-separated list of Key=Value pairs.\",\n",
    "        default = \"\",\n",
    "        help = \"\"\"Any comma-separated KEY=var from run.prm; e.g.:\n",
    "        -u HOME_MCCE=/path/to/mcce_home,H2O_SASCUTOFF=0.05,EXTRA=./extra.tpl; default: %(default)s.\n",
    "        Note: No space after a comma!\"\"\"},\n",
    "\n",
    "    #sub1.add_argument(\n",
    "    #    \"-msout_file\",\n",
    "    #    type = str,\n",
    "    #    default = \"pH7eH0ms.txt\",\n",
    "    #    help = \"Name of the mcce_dir/ms_out/ microstates file, `pHXeHYms.txt'; default: %(default)s.\"\"\",\n",
    "    #)\n",
    "\n",
    "    # bind sub1 parser with its related function:\n",
    "    sub1.set_defaults(func=bench_from_step1)\n",
    "\n",
    "    # later:\n",
    "    #sub2 = subparsers.add_parser(SUB_CMD2,\n",
    "    #                              formatter_class = RawDescriptionHelpFormatter,\n",
    "    #                              help=HELP_2)\n",
    "\n",
    "    return p\n",
    "\n",
    "\n",
    "pp(mcce_step_options)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "test_write_run_script_output()\n",
    "test_write_run_script()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# FUTURE\n",
    "new_sh = job_setup.write_run_script_from_template(benchmarks_dir,\n",
    "                                                  job_name = \"new_echo\",\n",
    "                                                  script_template=job_setup.ScriptChoices.TEST_ECHO)\n",
    "!cat {new_sh}"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "from enum import StrEnum\n",
    "\n",
    "\n",
    "RUN_SH_DEFAULTS = f\"\"\"#!/bin/bash\n",
    "step1.py --dry prot.pdb\n",
    "step2.py -d {MCCE_EPS}\n",
    "step3.py -d {MCCE_EPS}\n",
    "step4.py\n",
    "\n",
    "sleep {N_SLEEP}\n",
    "\"\"\"\n",
    "\n",
    "RUN_SH_TEST_ECHO = \"\"\"#!/bin/bash\n",
    "\n",
    "echo \"Using RUN_SH_TEST_ECHO as script: $PWD\"\n",
    "\"\"\"\n",
    "\n",
    "# template: expect dict for each step + sleep\n",
    "RUN_SH_TPL = \"\"\"#!/bin/bash\n",
    "step1.py --dry prot.pdb {}\n",
    "step2.py {}\n",
    "step3.py {}\n",
    "step4.py {}\n",
    "\n",
    "sleep {}\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "class RUN_SH(StrEnum):\n",
    "    # for testing bacth_run, only echo script pwd; ready to run:\n",
    "    TEST_ECHO = RUN_SH_TEST_ECHO\n",
    "\n",
    "    # preset with mcce defaults; ready to run:\n",
    "    SH_DEFAULTS = RUN_SH_DEFAULTS\n",
    "\n",
    "    # f-str template for user-args; expects dict for each step + sleep;\n",
    "    # populated using .format(<args>):\n",
    "    SH_TPL = RUN_SH_TPL\n",
    "\n",
    "\n",
    "def write_run_script(user_bench_folder:str,\n",
    "                     job_name:str,\n",
    "                     steps_options_dict:dict = None,\n",
    "                     sh_template:str = None) -> None:\n",
    "    \"\"\"Write a shell script in user_bench_folder/job_name/clean_pdbs\n",
    "    similar to RUN_SH_DEFAULTS\".\n",
    "\n",
    "    Target path: user_bench_folder/job_name/clean_pdbs/.\n",
    "    \"\"\"\n",
    "    target_fp = Path(user_bench_folder).joinpath(job_name, BENCH.CLEAN_PDBS)\n",
    "    if not target_fp.is_dir():\n",
    "        target_fp.mkdir()\n",
    "\n",
    "    if sh_template == RUN_SH.TEST_ECHO.value:\n",
    "        with open(target_fp.joinpath(f\"{job_name}.sh\"), \"w\") as fsh:\n",
    "            fsh.writelines(sh_template)\n",
    "    else:\n",
    "        NotImplemented\n",
    "\n",
    "    return\n",
    "\n",
    "import subprocess\n",
    "import tempfile\n",
    "import time\n",
    "\n",
    "def test_write_run_script() -> bool:\n",
    "\n",
    "    user_bench_folder = Path.cwd()  #tempfile.mkdtemp()\n",
    "    job_name = \"echo_job\"\n",
    "    user_job_folder = user_bench_folder.joinpath(job_name)\n",
    "    if not user_job_folder.is_dir():\n",
    "        user_job_folder.mkdir()\n",
    "\n",
    "    target_fp = user_job_folder.joinpath(BENCH.CLEAN_PDBS)\n",
    "    if not target_fp.is_dir():\n",
    "        target_fp.mkdir()\n",
    "\n",
    "    write_run_script(user_bench_folder,\n",
    "                     job_name,\n",
    "                     sh_template = RUN_SH.TEST_ECHO.value)\n",
    "\n",
    "    #time.sleep(3)\n",
    "\n",
    "    return target_fp.joinpath(f\"{job_name}.sh\").exists()\n",
    "\n",
    "\n",
    "def test_write_run_script_output() -> bool:\n",
    "\n",
    "    user_bench_folder = Path.cwd()  #tempfile.mkdtemp()\n",
    "    job_name = \"echo_job\"\n",
    "    user_job_folder = user_bench_folder.joinpath(job_name)\n",
    "    if not user_job_folder.is_dir():\n",
    "        user_job_folder.mkdir()\n",
    "\n",
    "    target_fp = user_job_folder.joinpath(BENCH.CLEAN_PDBS)\n",
    "    if not target_fp.is_dir():\n",
    "        target_fp.mkdir()\n",
    "\n",
    "    write_run_script(user_bench_folder,\n",
    "                     job_name,\n",
    "                     sh_template = RUN_SH.TEST_ECHO.value)\n",
    "\n",
    "    #os.chdir(user_job_folder.name)\n",
    "    #os.chdir(BENCH.CLEAN_PDBS)\n",
    "    os.chdir(target_fp)\n",
    "    print(Path.cwd())\n",
    "\n",
    "    job_script = f\"{job_name}.sh\"\n",
    "\n",
    "    try:\n",
    "        p = subprocess.run(f\"{sh_path}\",\n",
    "                           capture_output=True,\n",
    "                           check=True,\n",
    "                           shell=True,\n",
    "                           )\n",
    "       echo_path = Path(p.stdout.strip())\n",
    "\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        os.chdir(\"../../\")\n",
    "        print(f\"Error in subprocess cmd 'chmod +x':\\nException: {e}\")\n",
    "        raise\n",
    "\n",
    "    os.chdir(\"../../\")\n",
    "\n",
    "\n",
    "    return target_fp == echo_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Test cli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## data_setup"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Warning: Do not rerun if you have already setup your <benchmark_dir> with a reduced set:\n",
    "# it will be replaced with the full set.\n",
    "\n",
    "cmd = f\"data_setup -benchmarks_dir {benchmarks_dir}\".split()\n",
    "cmd\n",
    "args = cli_parser.parse_args(cmd)\n",
    "args.func(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## script_setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mcce_benchmark import custom_sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO]: mcce_benchmark.cli, bench_script_setup:\n",
      "\tmccebench args:\n",
      "{'subparser_name': 'script_setup',\n",
      " 'benchmarks_dir': PosixPath('/home/cat/projects/mcce_benchmarks_test'),\n",
      " 'job_name': 'e8_run',\n",
      " 'sentinel_file': 'pK.out',\n",
      " 'wet': False,\n",
      " 'noter': False,\n",
      " 'u': '',\n",
      " 's1_norun': False,\n",
      " 's2_norun': False,\n",
      " 's3_norun': False,\n",
      " 's4_norun': False,\n",
      " 'd': 8.0,\n",
      " 'conf_making_level': 1,\n",
      " 'c': [1, 99999],\n",
      " 'x': 'delphi',\n",
      " 'f': '/tmp',\n",
      " 'p': 1,\n",
      " 'r': False,\n",
      " 'titr_type': 'ph',\n",
      " 'i': 0.0,\n",
      " 'interval': 1.0,\n",
      " 'n': 15,\n",
      " 'ms': False,\n",
      " 'func': <function mcce_benchmark.cli.bench_script_setup(args: argparse.Namespace) -> None>}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO]: mcce_benchmark.cli, bench_script_setup:\n",
      "\tmccebench args:\n",
      "{'subparser_name': 'script_setup',\n",
      " 'benchmarks_dir': PosixPath('/home/cat/projects/mcce_benchmarks_test'),\n",
      " 'job_name': 'e8_run',\n",
      " 'sentinel_file': 'pK.out',\n",
      " 'wet': False,\n",
      " 'noter': False,\n",
      " 'u': '',\n",
      " 's1_norun': False,\n",
      " 's2_norun': False,\n",
      " 's3_norun': False,\n",
      " 's4_norun': False,\n",
      " 'd': 8.0,\n",
      " 'conf_making_level': 1,\n",
      " 'c': [1, 99999],\n",
      " 'x': 'delphi',\n",
      " 'f': '/tmp',\n",
      " 'p': 1,\n",
      " 'r': False,\n",
      " 'titr_type': 'ph',\n",
      " 'i': 0.0,\n",
      " 'interval': 1.0,\n",
      " 'n': 15,\n",
      " 'ms': False,\n",
      " 'func': <function mcce_benchmark.cli.bench_script_setup(args: argparse.Namespace) -> None>}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO]: mcce_benchmark.cli, bench_script_setup:\n",
      "\tWrite fresh book file.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO]: mcce_benchmark.cli, bench_script_setup:\n",
      "\tWrite fresh book file.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO]: mcce_benchmark.audit, list_all_valid_pdbs_dirs:\n",
      "\tValid folders: 120; Invalid folders: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO]: mcce_benchmark.audit, list_all_valid_pdbs_dirs:\n",
      "\tValid folders: 120; Invalid folders: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO]: mcce_benchmark.cli, bench_script_setup:\n",
      "\tWriting script for 'e8_run' job.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO]: mcce_benchmark.cli, bench_script_setup:\n",
      "\tWriting script for 'e8_run' job.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO]: mcce_benchmark.cli, bench_script_setup:\n",
      "\tDeleting previous sentinel files, if any.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO]: mcce_benchmark.cli, bench_script_setup:\n",
      "\tDeleting previous sentinel files, if any.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO]: mcce_benchmark.job_setup, delete_sentinel:\n",
      "\t120 sentinel file(s) deleted. Sentinel: 'pK.out'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO]: mcce_benchmark.job_setup, delete_sentinel:\n",
      "\t120 sentinel file(s) deleted. Sentinel: 'pK.out'\n"
     ]
    }
   ],
   "source": [
    "# script setup: includes deletion of 'sentinel file' (i.e. pK.out or step2_out.pdb)\n",
    "\n",
    "job = \"e8_run\"\n",
    "\n",
    "cmd = f\"script_setup -benchmarks_dir {benchmarks_dir} -job_name {job} -d 8\".split()\n",
    "args = cli_parser.parse_args(cmd)\n",
    "args.func(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "PosixPath('/home/cat/projects/mcce_benchmarks_test/clean_pdbs/e8_run.sh')"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " #!/bin/bash\n",
      "\n",
      " step1.py prot.pdb --dry -d 8.0\n",
      " step2.py -d 8.0\n",
      " step3.py -d 8.0\n",
      " step4.py --xts \n",
      "\n",
      " sleep 10\n"
     ]
    }
   ],
   "source": [
    "sh_path = benchmarks_dir.joinpath(BENCH.CLEAN_PDBS, f\"{job}.sh\")\n",
    "sh_path.exists()\n",
    "sh_path\n",
    "!cat {sh_path}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "PosixPath('/home/cat/projects/mcce_benchmarks_test/clean_pdbs/norun_foo.sh')"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "#!/bin/bash\n",
      "\n",
      "step1.py prot.pdb --norun\n"
     ]
    }
   ],
   "source": [
    "job = \"norun_foo\"\n",
    "custom_sh.write_run_script_from_template(benchmarks_dir, job,\n",
    "                                                    script_template = ScriptChoices.NORUN,\n",
    "                                                    job_args = None)\n",
    "sh_path = benchmarks_dir.joinpath(BENCH.CLEAN_PDBS, f\"{job}.sh\")\n",
    "sh_path.exists()\n",
    "sh_path\n",
    "!cat {sh_path}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " #!/bin/bash\n",
      "\n",
      " step1.py prot.pdb --dry -d 8.0\n",
      " step2.py -d 8.0\n",
      " step3.py -d 8.0\n",
      " step4.py --xts \n",
      "\n",
      " sleep 10\n"
     ]
    }
   ],
   "source": [
    "!cat {sh_path}"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from enum import Enum\n",
    "from typing import Union\n",
    "from mcce_benchmark.custom_sh import RUN_SH_TEST_ECHO, RUN_SH_NORUN, SH_TEMPLATE, check_steps_opt_defaults\n",
    "\n",
    "class ScriptChoices(Enum):\n",
    "    TEST_ECHO = RUN_SH_TEST_ECHO\n",
    "    NORUN = RUN_SH_NORUN\n",
    "    CUSTOM = SH_TEMPLATE\n",
    "\n",
    "\n",
    "cli_to_mcce_opt = {\"wet\":\"dry\",\n",
    "                   \"conf_making_level\":\"l\",\n",
    "                   \"interval\":\"d\",\n",
    "                   \"titr_type\":\"t\",\n",
    "                   \"s1_norun\":\"norun\",\n",
    "                   \"s2_norun\":\"norun\",\n",
    "                   \"s3_norun\":\"norun\",\n",
    "                   \"s4_norun\":\"norun\",\n",
    "                  }\n",
    "\n",
    "# cli defaults per step:\n",
    "defaults_per_step = {\n",
    "\"s1\": {\"wet\":False, \"noter\":False, \"d\":4.0, \"norun\":False, \"u\":\"\"},\n",
    "\"s2\": {\"conf_making_level\":1, \"d\":4.0, \"norun\":False, \"u\":\"\"},\n",
    "\"s3\": {\"c\":[1, 99999], \"x\":\"delphi\", \"f\":\"/tmp\", \"p\":1, \"r\":False, \"d\":4.0, \"norun\":False, \"u\":\"\"},\n",
    "\"s4\": {\"titr_type\":\"ph\", \"i\":0.0, \"interval\":1.0, \"n\":15, \"ms\":False, \"norun\":False, \"u\":\"\"},\n",
    "}\n",
    "# combined:\n",
    "all_default_opts = {}\n",
    "for S in defaults_per_step:\n",
    "    all_default_opts.update(((k, v) for k, v in defaults_per_step[S].items()))\n",
    "\n",
    "\n",
    "def check_steps_opt_defaultsX(sh_args:argNamespace) -> tuple():\n",
    "    \"\"\"Purpose: to determine whether to write a custom script or use the default one.\n",
    "    If sh_args are default for all the steps: return (True, None), else return (False, sh_args),\n",
    "    \"\"\"\n",
    "\n",
    "    # keep mcce options:\n",
    "    excluded_keys = [\"subparser_name\",\"benchmarks_dir\",\"sentinel_file\", \"job_name\",\"func\"]\n",
    "    d_sh_args = {k:v for k, v in vars(sh_args).items() if k not in excluded_keys}\n",
    "\n",
    "    is_default = True\n",
    "    for opt in d_sh_args:\n",
    "        print(f\"{opt = }, {d_sh_args[opt] = }; {all_default_opts.get(opt) = }\")\n",
    "        is_default = is_default and d_sh_args[opt] == all_default_opts.get(opt)\n",
    "        if not is_default:  # done\n",
    "            return False, sh_args\n",
    "\n",
    "    return True, None\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "use_default_script = check_steps_opt_defaults(args)\n",
    "use_default_script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('[1, 99999]', [1, 99999])"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = [1, 99999]\n",
    "\n",
    "y = str(x)\n",
    "y, x"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "from enum import Enum\n",
    "from mcce_benchmark.custom_sh import cli_args_to_dict\n",
    "\n",
    "# To check script is run inside a PDB folder:\n",
    "RUN_SH_TEST_ECHO = \"\"\"\n",
    "#!/bin/bash\n",
    "\n",
    "echo \"Using RUN_SH_TEST_ECHO as script: $PWD\"\n",
    "\"\"\"\n",
    "# To test submit_script without running anything:\n",
    "RUN_SH_NORUN = \"\"\"\n",
    "#!/bin/bash\n",
    "\n",
    "step1.py prot.pdb --norun\n",
    "\"\"\"\n",
    "#...............................................................................\n",
    "# for custom script:\n",
    "SH_TEMPLATE = \"\"\"\n",
    " #!/bin/bash\n",
    "\n",
    " step1.py prot.pdb {wet}{noter}{d}{s1_norun}{u}\n",
    " step2.py {conf_making_level}{d}{s2_norun}{u}\n",
    " step3.py {c}{x}{f}{p}{r}{d}{s3_norun}{u}\n",
    " step4.py --xts {titr_type}{i}{interval}{n}{ms}{s4_norun}{u}\n",
    "\n",
    " sleep 10\n",
    "\"\"\"\n",
    "\n",
    "class ScriptChoices(Enum):\n",
    "    TEST_ECHO = RUN_SH_TEST_ECHO\n",
    "    NORUN = RUN_SH_NORUN\n",
    "    CUSTOM = SH_TEMPLATE\n",
    "\n",
    "cli_to_mcce_opt = {\"wet\":\"dry\",\n",
    "                   \"conf_making_level\":\"l\",\n",
    "                   \"interval\":\"d\",\n",
    "                   \"titr_type\":\"t\",\n",
    "                   \"s1_norun\":\"norun\",\n",
    "                   \"s2_norun\":\"norun\",\n",
    "                   \"s3_norun\":\"norun\",\n",
    "                   \"s4_norun\":\"norun\",\n",
    "                  }\n",
    "# cli defaults per step:\n",
    "defaults_per_step = {\n",
    "\"s1\": {\"wet\":False, \"noter\":False, \"d\":4.0, \"s1_norun\":False, \"u\":\"\"},\n",
    "\"s2\": {\"conf_making_level\":1, \"d\":4.0, \"s2_norun\":False, \"u\":\"\"},\n",
    "\"s3\": {\"c\":[1, 99999], \"x\":\"delphi\", \"f\":\"/tmp\", \"p\":1, \"r\":False, \"d\":4.0, \"s3_norun\":False, \"u\":\"\"},\n",
    "\"s4\": {\"titr_type\":\"ph\", \"i\":0.0, \"interval\":1.0, \"n\":15, \"ms\":False, \"s4_norun\":False, \"u\":\"\"},\n",
    "}\n",
    "# combined:\n",
    "all_default_opts = {}\n",
    "for S in defaults_per_step:\n",
    "    all_default_opts.update(((k, v) for k, v in defaults_per_step[S].items()))\n",
    "\n",
    "\n",
    "def populate_custom_template(job_args:argNamespace) -> str:\n",
    "    \"\"\"Return the custom template string filled with appropriate values.\"\"\"\n",
    "\n",
    "    d_args = cli_args_to_dict(job_args)\n",
    "    d_all = {}\n",
    "    # note: trailing spaces needed:\n",
    "    v = d_args.pop(\"wet\")\n",
    "    d_all[\"wet\"] = \"\" if v else \"--dry \"\n",
    "    v = d_args.pop(\"noter\")\n",
    "    d_all[\"noter\"] = \"--noter \" if v else \"\"\n",
    "    for s in [\"s1_norun\",\"s2_norun\",\"s3_norun\",\"s4_norun\"]:\n",
    "        v = d_args.pop(s)\n",
    "        d_all[s] = \"--norun \" if v else \"\"\n",
    "    # all remaining:\n",
    "    for k in d_args:\n",
    "        v = d_args.get(k, \"\")\n",
    "        if str(v) == str(all_default_opts[k]):\n",
    "            d_all[k] = \"\"\n",
    "        else:\n",
    "            d_all[k] = f\"-{cli_to_mcce_opt.get(k, k) } {v }\"\n",
    "\n",
    "    body = ScriptChoices.CUSTOM.value.format(**d_all)\n",
    "\n",
    "    return body\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " #!/bin/bash\n",
      "\n",
      " step1.py prot.pdb --dry -d 8.0\n",
      " step2.py -d 8.0\n",
      " step3.py -d 8.0\n",
      " step4.py --xts \n",
      "\n",
      " sleep 10\n",
      "\n"
     ]
    }
   ],
   "source": [
    "out = populate_custom_template(args)\n",
    "print(out)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NEW req: conda env required (for scheduling)\n",
    "-> MOVE check_env() to `__ini__.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['launch_batch',\n",
       " '-benchmarks_dir',\n",
       " '/home/cat/projects/mcce_benchmarks',\n",
       " '-job_name',\n",
       " 'dummy',\n",
       " '-n_active',\n",
       " '3']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO]: mcce_benchmark.cli, bench_launch_batch:\n",
      "\tmccebench args:\n",
      "{'subparser_name': 'launch_batch',\n",
      " 'benchmarks_dir': PosixPath('/home/cat/projects/mcce_benchmarks'),\n",
      " 'job_name': 'dummy',\n",
      " 'n_active': 3,\n",
      " 'sentinel_file': 'pK.out',\n",
      " 'func': <function mcce_benchmark.cli.bench_launch_batch(args: argparse.Namespace) -> None>}\n",
      "\n",
      "[INFO]: mcce_benchmark.cli, bench_launch_batch:\n",
      "\tSubmiting batch of jobs.\n",
      "[INFO]: mcce_benchmark.cli, bench_launch_batch:\n",
      "\tDebug mode: batch_submit.launch_job not called\n"
     ]
    }
   ],
   "source": [
    "# launch_batch: now required the name of the conda env where the cli was installed\n",
    "\n",
    "mc_env = check_env()\n",
    "n_jobs_to_maintain = 3\n",
    "\n",
    "cmd = f\"launch_batch -conda_env {mc_env} -benchmarks_dir {benchmarks_dir} -job_name {job} -n_active {n_jobs_to_maintain}\".split()\n",
    "cmd\n",
    "args = cli_parser.parse_args(cmd)\n",
    "args.func(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "---\n",
    "# Packaged data management\n",
    "## Prep of the \"master\" pdbs folder, `BENCH_PDBS`:\n",
    " * Remove any MCCE output files or folder along with prot.pdb\n",
    "---"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from benchmark import audit, cleanup\n",
    "\n",
    "cleanup.clean_job_folder(BENCH.BENCH_PDBS)\n",
    "audit.reset_book_file(BENCH.BENCH_Q_BOOK)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "def main():\n",
    "    \"\"\"main fn of pkanalysis.py\"\"\"\n",
    "\n",
    "    calc_pkas = job_pkas_to_dict()\n",
    "    expr_pkas = experimental_pkas_to_dict()\n",
    "    matched_pks = match_pkas(expr_pkas, calc_pkas)\n",
    "    n = len(matched_pks)\n",
    "    matched_pkas_to_csv(matched_pks)\n",
    "\n",
    "    # Overall fitting\n",
    "    x = np.array([p[1] for p in matched_pks])  # x: experiemntal pKas\n",
    "    y = np.array([p[2] for p in matched_pks])  # y: calculated pKas\n",
    "\n",
    "    b, m = np.polynomial.Polynomial.fit(x, y, 1, domain=[0,20]).convert().coef\n",
    "    op = \"+\" if m > 0 else \"-\"\n",
    "    print(f\"y (calculated pKa) = {b:.3f} {op} {abs(m):.3f}x (experimental pKa)\")\n",
    "\n",
    "    delta = x - y\n",
    "    rmsd = np.sqrt(np.mean(delta**2))\n",
    "    print(f\"RMSD between expl. and calc. = {rmsd:.3f}\")\n",
    "\n",
    "    within_2, within_1 = 0, 0\n",
    "    for d in np.abs(delta):\n",
    "        if d <= 2.0:\n",
    "            within_2 += 1\n",
    "            if d <= 1.0:\n",
    "                within_1 += 1\n",
    "\n",
    "    print(f\"{within_2/n:.1%} within 2 pH units\")\n",
    "    print(f\"{within_1/n:.1%} within 1 pH unit\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matched_pks = []\n",
    "for i in range(10):\n",
    "    matched_pks.append((random.choice(\"ABCDRGWSX\"),\n",
    "                         random.choice([3.2, 5.1, 6., 4.4, 7.2]),\n",
    "                        random.choice([3.2, 5.1, 6., 4.4, 7.2]*2)))\n",
    "matched_pks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pka_dict = experimental_pkas_to_dict(WT)\n",
    "len(pka_dict)\n",
    "list(pka_dict.keys())[:10]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:mce]",
   "language": "python",
   "name": "conda-env-mce-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
