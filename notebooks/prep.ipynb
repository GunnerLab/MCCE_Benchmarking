{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Run the first 2 code cells without modifications_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python ver: 3.11.5 | packaged by conda-forge | (main, Aug 27 2023, 03:34:09) [GCC 12.3.0]\n",
      "Python env: mce\n",
      "Currrent dir: /home/cat/projects/MCCE_Benchmarking/notebooks\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "from IPython.display import Markdown #, IFrame\n",
    "# for presentations:\n",
    "#display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "#pd.set_option(\"display.max_colwidth\", 200)\n",
    "import matplotlib as mpl\n",
    "from matplotlib import pyplot as plt\n",
    "plt.ion()\n",
    "plt.style.use('seaborn-v0_8-muted')\n",
    "from pprint import pprint as ptp\n",
    "\n",
    "import sys\n",
    "from pathlib import Path\n",
    "print('Python ver: {}\\nPython env: {}'.format(sys.version, Path(sys.prefix).name))\n",
    "print('Currrent dir: {}\\n'.format(Path.cwd()))\n",
    "\n",
    "\n",
    "def add_to_sys_path(this_path, up=False):\n",
    "    \"\"\"\n",
    "    Prepend this_path to sys.path.\n",
    "    If up=True, path refers to parent folder (1 level up).\n",
    "    \"\"\"\n",
    "\n",
    "    if up:\n",
    "        newp = str(Path(this_path).parent)\n",
    "    else:\n",
    "        newp = str(Path(this_path))\n",
    "    if newp not in sys.path:\n",
    "        sys.path.insert(1, newp)\n",
    "        print('Path added to sys.path: {}'.format(newp))\n",
    "\n",
    "\n",
    "def fdir(obj, start_with_str='_', exclude=True):\n",
    "    \"\"\"Filtered dir() for method discovery.\"\"\"\n",
    "    return [d for d in dir(obj) if not d.startswith(start_with_str) == exclude]\n",
    "\n",
    "# autoreload extension\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path added to sys.path: /home/cat/projects/MCCE_Benchmarking\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PosixPath('/home/cat/projects/MCCE_Benchmarking/notebooks')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "add_to_sys_path(Path.cwd(), up=True)\n",
    "notebooks_dir = Path.cwd()\n",
    "notebooks_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-02-02 11:37:45,319 @cat [INFO: mcce_benchmark]: - START\n",
      "======================================================================\n",
      "APP VER: (0, 1, 'dev30', 'g23b1e74.d20240129')\n",
      "APP DEFAULTS:\n",
      "    Globals: MCCE_EPS = 4; N_SLEEP = 10; N_ACTIVE = 10\n",
      "    Default names:\n",
      "    DEFAULT_DIR = 'mcce_benchmarks'\n",
      "    BENCH.CLEAN_PDBS = 'clean_pdbs'\n",
      "    BENCH.Q_BOOK = 'book.txt'\n",
      "    BENCH.DEFAULT_JOB = 'default_run'\n",
      "======================================================================\n",
      "\n",
      "\n",
      "        BENCH_DATA = /home/cat/projects/MCCE_Benchmarking/mcce_benchmark/data\n",
      "        BENCH_WT = /home/cat/projects/MCCE_Benchmarking/mcce_benchmark/data/WT_pkas.csv\n",
      "        BENCH_PROTS = /home/cat/projects/MCCE_Benchmarking/mcce_benchmark/data/proteins.tsv\n",
      "        BENCH_PDBS = /home/cat/projects/MCCE_Benchmarking/mcce_benchmark/data/clean_pdbs\n",
      "        DEFAULT_JOB = default_run\n",
      "        DEFAULT_JOB_SH = /home/cat/projects/MCCE_Benchmarking/mcce_benchmark/data/clean_pdbs/default_run.sh\n",
      "        BENCH_Q_BOOK = /home/cat/projects/MCCE_Benchmarking/mcce_benchmark/data/clean_pdbs/book.txt\n",
      "        CLEAN_PDBS = clean_pdbs\n",
      "        Q_BOOK = book.txt\n",
      "        \n"
     ]
    }
   ],
   "source": [
    "from mcce_benchmark import BENCH, DEFAULT_DIR, APP_NAME, MCCE_EPS, N_SLEEP, N_ACTIVE\n",
    "from mcce_benchmark import audit, job_setup\n",
    "\n",
    "print(BENCH)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from importlib import resources\n",
    "\n",
    "res_files = resources.files(f\"{APP_NAME}.data\")\n",
    "res_files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Future Job Setup?"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# FUTURE\n",
    "new_sh = job_setup.write_run_script_from_template(benchmarks_dir,\n",
    "                                                  job_name = \"new_echo\",\n",
    "                                                  script_template=job_setup.ScriptChoices.TEST_ECHO)\n",
    "!cat {new_sh}"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "from enum import StrEnum\n",
    "\n",
    "\n",
    "RUN_SH_DEFAULTS = f\"\"\"#!/bin/bash\n",
    "step1.py --dry prot.pdb\n",
    "step2.py -d {MCCE_EPS}\n",
    "step3.py -d {MCCE_EPS}\n",
    "step4.py\n",
    "\n",
    "sleep {N_SLEEP}\n",
    "\"\"\"\n",
    "\n",
    "RUN_SH_TEST_ECHO = \"\"\"#!/bin/bash\n",
    "\n",
    "echo \"Using RUN_SH_TEST_ECHO as script: $PWD\"\n",
    "\"\"\"\n",
    "\n",
    "# template: expect dict for each step + sleep\n",
    "RUN_SH_TPL = \"\"\"#!/bin/bash\n",
    "step1.py --dry prot.pdb {}\n",
    "step2.py {}\n",
    "step3.py {}\n",
    "step4.py {}\n",
    "\n",
    "sleep {}\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "class RUN_SH(StrEnum):\n",
    "    # for testing bacth_run, only echo script pwd; ready to run:\n",
    "    TEST_ECHO = RUN_SH_TEST_ECHO\n",
    "\n",
    "    # preset with mcce defaults; ready to run:\n",
    "    SH_DEFAULTS = RUN_SH_DEFAULTS\n",
    "\n",
    "    # f-str template for user-args; expects dict for each step + sleep;\n",
    "    # populated using .format(<args>):\n",
    "    SH_TPL = RUN_SH_TPL\n",
    "\n",
    "\n",
    "def write_run_script(user_bench_folder:str,\n",
    "                     job_name:str,\n",
    "                     steps_options_dict:dict = None,\n",
    "                     sh_template:str = None) -> None:\n",
    "    \"\"\"Write a shell script in user_bench_folder/job_name/clean_pdbs\n",
    "    similar to RUN_SH_DEFAULTS\".\n",
    "\n",
    "    Target path: user_bench_folder/job_name/clean_pdbs/.\n",
    "    \"\"\"\n",
    "    target_fp = Path(user_bench_folder).joinpath(job_name, BENCH.CLEAN_PDBS)\n",
    "    if not target_fp.is_dir():\n",
    "        target_fp.mkdir()\n",
    "\n",
    "    if sh_template == RUN_SH.TEST_ECHO.value:\n",
    "        with open(target_fp.joinpath(f\"{job_name}.sh\"), \"w\") as fsh:\n",
    "            fsh.writelines(sh_template)\n",
    "    else:\n",
    "        NotImplemented\n",
    "\n",
    "    return\n",
    "\n",
    "import subprocess\n",
    "import tempfile\n",
    "import time\n",
    "\n",
    "def test_write_run_script() -> bool:\n",
    "\n",
    "    user_bench_folder = Path.cwd()  #tempfile.mkdtemp()\n",
    "    job_name = \"echo_job\"\n",
    "    user_job_folder = user_bench_folder.joinpath(job_name)\n",
    "    if not user_job_folder.is_dir():\n",
    "        user_job_folder.mkdir()\n",
    "\n",
    "    target_fp = user_job_folder.joinpath(BENCH.CLEAN_PDBS)\n",
    "    if not target_fp.is_dir():\n",
    "        target_fp.mkdir()\n",
    "\n",
    "    write_run_script(user_bench_folder,\n",
    "                     job_name,\n",
    "                     sh_template = RUN_SH.TEST_ECHO.value)\n",
    "\n",
    "    #time.sleep(3)\n",
    "\n",
    "    return target_fp.joinpath(f\"{job_name}.sh\").exists()\n",
    "\n",
    "\n",
    "def test_write_run_script_output() -> bool:\n",
    "\n",
    "    user_bench_folder = Path.cwd()  #tempfile.mkdtemp()\n",
    "    job_name = \"echo_job\"\n",
    "    user_job_folder = user_bench_folder.joinpath(job_name)\n",
    "    if not user_job_folder.is_dir():\n",
    "        user_job_folder.mkdir()\n",
    "\n",
    "    target_fp = user_job_folder.joinpath(BENCH.CLEAN_PDBS)\n",
    "    if not target_fp.is_dir():\n",
    "        target_fp.mkdir()\n",
    "\n",
    "    write_run_script(user_bench_folder,\n",
    "                     job_name,\n",
    "                     sh_template = RUN_SH.TEST_ECHO.value)\n",
    "\n",
    "    #os.chdir(user_job_folder.name)\n",
    "    #os.chdir(BENCH.CLEAN_PDBS)\n",
    "    os.chdir(target_fp)\n",
    "    print(Path.cwd())\n",
    "\n",
    "    job_script = f\"{job_name}.sh\"\n",
    "\n",
    "    try:\n",
    "        p = subprocess.run(f\"{sh_path}\",\n",
    "                           capture_output=True,\n",
    "                           check=True,\n",
    "                           shell=True,\n",
    "                           )\n",
    "       echo_path = Path(p.stdout.strip())\n",
    "\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        os.chdir(\"../../\")\n",
    "        print(f\"Error in subprocess cmd 'chmod +x':\\nException: {e}\")\n",
    "        raise\n",
    "\n",
    "    os.chdir(\"../../\")\n",
    "\n",
    "\n",
    "    return target_fp == echo_path"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "test_write_run_script_output()\n",
    "test_write_run_script()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "mcce_step_options = {\n",
    "    \"S1\":{\"msg\":\"Run mcce step 1, premcce to format PDB file to MCCE PDB format.\",\n",
    "          \"--noter\": {\"default\":False, \"help\":\"Do not label terminal residues (for making ftpl).\", \"action\":\"store_true\"},\n",
    "          \"--dry\":   {\"default\":False, \"help\":\"Delete all water molecules.\", \"action\":\"store_true\"},\n",
    "          },\n",
    "    \"S2\":{\"msg\":\"Run mcce step 2, make side chain conformers from step1_out.pdb.\",\n",
    "          \"-l\":      {\"metavar\":\"level\",\n",
    "                      \"type\":int, \"default\":1,\n",
    "                      \"help\":\"Conformer level 1=quick (default), 2=medium, 3=full\"},\n",
    "          },\n",
    "    \"S3\":{\"msg\":\"Run mcce step 3, energy calculations, with multiple threads.\",\n",
    "          # should have been --r:\n",
    "          \"-r\":      {\"default\":False, \"help\":\"refresh opp files and head3.lst without running delphi\", \"action\":\"store_true\"},\n",
    "          \"-c\":      {\"metavar\":\"('conf start', 'conf end')\",\n",
    "                      \"type\":int,\n",
    "                      \"default\":[1, 99999], \"nargs\":2,\n",
    "                       \"help\":\"starting and ending conformer, default to 1 and 9999\"},\n",
    "          \"-f\":      {\"metavar\":\"tmp folder\", \"default\":\"/tmp\", \"hel\":\"delphi temporary folder, default to /tmp\"},\n",
    "          \"-p\":      {\"metavar\":\"processes\", \"type\":int, \"default\":1,\n",
    "                      \"help\":\"run mcce with p number of processes; default: %(default)s.\"},\n",
    "          },\n",
    "    \"S4\":{\"msg\":\"Run mcce step 4, Monte Carlo sampling to simulate a titration.\",\n",
    "          \"--xts\":   {\"default\":False, \"help\":\"Enable entropy correction, default is false\", \"action\":\"store_true\"},\n",
    "          \"--ms\":    {\"default\":False, \"help\":\"Enable microstate output\", \"action\":\"store_true\"},\n",
    "          \"-t\":      {\"metavar\":\"ph or eh\", \"default\":\"ph\", \"help\":\"titration type: pH or Eh.\"},\n",
    "          \"-i\":      {\"metavar\":\"initial ph/eh\", \"default\":\"0.0\", \"help\":\"Initial pH/Eh of titration; default: %(default)s.\"},\n",
    "          \"-d\":      {\"metavar\":\"interval\", \"default\":\"1.0\", \"help\":\"titration interval in pJ or mV; default: %(default)s.\"},\n",
    "          \"-n\":      {\"metavar\":\"steps\", \"default\":\"15\", \"help\":\"number of steps of titration; default: %(default)s.\"},\n",
    "          }\n",
    "}\n",
    "\n",
    "\n",
    "CLI_NAME = \"mcce_bench\"  # as per pyproject.toml\n",
    "SUB_CMD1, SUB_CMD2 = \"from_step1\", \"from_step3\"\n",
    "USAGE = f\"{CLI_NAME} <sub-command for simulation start> <related args>\\n\"\n",
    "\n",
    "DESC = f\"\"\"\n",
    "    Launch a MCCE benchmarking job using curated structures from the pKa Database v1.\n",
    "\n",
    "    The main command is {CLI_NAME!r} along with one of two sub-commands,\n",
    "    which distinguishes the starting point for the MCCE simulation.\n",
    "    - Sub-command {SUB_CMD1!r}: starts from step1 -> step4;\n",
    "    - Sub-command {SUB_CMD2!r}: starts from step3 -> step4 :: NOT YET IMPLEMENTED!\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "HELP_1 = f\"Sub-command {SUB_CMD1!r} for starting the MCCE simulation from step1.\"\n",
    "HELP_2 = f\"Sub-command {SUB_CMD2!r} for starting the MCCE simulation from step3.\"\n",
    "\n",
    "def bench_from_step1(args):\n",
    "    \"\"\"Benchmark setup and launch for 'from_step1' sub-command.\"\"\"\n",
    "    # TODO\n",
    "    # setup folders\n",
    "    # write <job_name>.sh\n",
    "    # launch\n",
    "    pass\n",
    "\n",
    "\n",
    "def bench_from_step3(args):\n",
    "    \"\"\"Benchmark setup and launch for 'from_step3' sub-command.\"\"\"\n",
    "    # TODO later\n",
    "    pass\n",
    "\n",
    "\n",
    "def bench_parser():\n",
    "    \"\"\"Command line arguments parser with sub-commands for use in benchmarking.\n",
    "    \"\"\"\n",
    "\n",
    "    def arg_valid_dirpath(p: str):\n",
    "        \"\"\"Return resolved path from the command line.\"\"\"\n",
    "        if not len(p):\n",
    "            return None\n",
    "        return Path(p).resolve()\n",
    "\n",
    "    p = ArgumentParser(\n",
    "        prog = f\"{CLI_NAME} \",\n",
    "        description = DESC,\n",
    "        usage = USAGE,\n",
    "        formatter_class = RawDescriptionHelpFormatter,\n",
    "        epilog = \">>> END of %(prog)s.\",\n",
    "    )\n",
    "    subparsers = p.add_subparsers(required=True,\n",
    "                                  title='pipeline step commands',\n",
    "                                  description='Subcommands of the MCCE-CDC processing pipeline',\n",
    "                                  help='The 3 steps of the MCCE-CDC processing pipeline',\n",
    "                                  dest='subparser_name'\n",
    "                                 )\n",
    "\n",
    "    # do_ms_to_pdbs\n",
    "    sub1 = subparsers.add_parser(SUB_CMD1,\n",
    "                                 formatter_class = RawDescriptionHelpFormatter,\n",
    "                                  help=HELP_1)\n",
    "    sub1.add_argument(\n",
    "        \"benchmark_dir\",\n",
    "        type = arg_valid_dirpath,\n",
    "        help = \"\"\"The user's choice of directory for setting up the benchmarking job(s); required.\n",
    "        If the directory does not exists in the location where this cli is called, then it is\n",
    "        created. Recommended name: \"mcce_benchmarks\"; this is where all subsequent jobs will\n",
    "        reside as subfolders.\n",
    "        \"\"\"\n",
    "    )\n",
    "    sub1.add_argument(\n",
    "        \"job_name\",\n",
    "        type = str,\n",
    "        help = \"\"\"The descriptive name, devoid of spaces, for the current job (don't make it too long!); required.\n",
    "        This job_name is be used to name the curent job in 'benchmark_dir' and name the script that launches the\n",
    "        MCCE simulation in ./clean_pdbs folder.\n",
    "        \"\"\"\n",
    "    )\n",
    "    # always 'prot.pdb' as per soft-link setup: ln -s DIR/dir.pdb prot.pdb\n",
    "    #sub1.add_argument(\n",
    "    #    \"-prot\",\n",
    "    #    metavar = \"pdb\",\n",
    "    #    default = \"prot.pdb\",\n",
    "    #    help = \"The name of the pdb; default: %(default)s.\",\n",
    "    )\n",
    "    sub1.add_argument(\n",
    "        \"--dry\",\n",
    "        default = False,\n",
    "        help = \"No water molecules.\",\n",
    "        action = \"store_true\"\n",
    "    )\n",
    "    sub1.add_argument(\n",
    "        \"--norun\",\n",
    "        default = False,\n",
    "        action = \"store_true\",\n",
    "        help = \"Create run.prm without running the step\"\n",
    "    )\n",
    "    sub1.add_argument(\n",
    "        \"-e\",\n",
    "        metavar = \"/path/to/mcce\",\n",
    "        default = \"mcce\",\n",
    "        help = \"Location of the mcce executable, i.e. which mcce; default: %(default)s.\",\n",
    "    )\n",
    "    sub1.add_argument(\n",
    "        \"-eps\",\n",
    "        metavar = \"epsilon\",\n",
    "        default = \"4.0\",\n",
    "        help = \"Protein dielectric constant; default: %(default)s.\",\n",
    "    )\n",
    "    sub1.add_argument(\n",
    "        \"-u\",\n",
    "        metavar = \"Comma-separated list of Key=Value pairs.\",\n",
    "        default = \"\",\n",
    "        help = \"\"\"Any comma-separated KEY=var from run.prm; e.g.:\n",
    "        -u HOME_MCCE=/path/to/mcce_home,H2O_SASCUTOFF=0.05,EXTRA=./extra.tpl; default: %(default)s.\n",
    "        Note: No space after a comma!\"\"\"},\n",
    "\n",
    "    #sub1.add_argument(\n",
    "    #    \"-msout_file\",\n",
    "    #    type = str,\n",
    "    #    default = \"pH7eH0ms.txt\",\n",
    "    #    help = \"Name of the mcce_dir/ms_out/ microstates file, `pHXeHYms.txt'; default: %(default)s.\"\"\",\n",
    "    #)\n",
    "\n",
    "    # bind sub1 parser with its related function:\n",
    "    sub1.set_defaults(func=bench_from_step1)\n",
    "\n",
    "    # later:\n",
    "    #sub2 = subparsers.add_parser(SUB_CMD2,\n",
    "    #                              formatter_class = RawDescriptionHelpFormatter,\n",
    "    #                              help=HELP_2)\n",
    "\n",
    "    return p\n",
    "\n",
    "\n",
    "pp(mcce_step_options)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Test cli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/home/cat/projects/mcce_benchmarks')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# in my ~/projects folder:\n",
    "benchmarks_dir = Path.cwd().parent.parent.joinpath(DEFAULT_DIR)\n",
    "benchmarks_dir"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from argparse import ArgumentParser, RawDescriptionHelpFormatter, Namespace as argNamespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mcce_benchmark import cli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "cli_parser = cli.bench_parser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: \n",
      "mccebench <+ sub-command :: one of [data_setup, script_setup, launch_batch]> <related args>\n",
      "\n",
      "Examples for current implementation (Beta):\n",
      "\n",
      "1. Data setup\n",
      " - Using defaults (benchmarks_dir= mcce_benchmarks):\n",
      "   >mccebench data_setup\n",
      "\n",
      " - Using a different folder name:\n",
      "   >mccebench data_setup -benchmarks_dir <different name>\n",
      "\n",
      "2. Script setup\n",
      " - Using defaults (benchmarks_dir= mcce_benchmarks; job_name= default_run):\n",
      "   >mccebench script_setup\n",
      "\n",
      " - Using non-default option(s):\n",
      "   >mccebench script_setup -job_name <my_job_name>\n",
      "   >mccebench script_setup -benchmarks_dir <different name> -job_name <my_job_name>\n",
      "\n",
      "3. Submit batch of jobs\n",
      " - Using defaults (benchmarks_dir= mcce_benchmarks;\n",
      "                   job_name= default_run;\n",
      "                   n_active= 10;\n",
      "                   sentinel_file= pK.out):\n",
      "   >mccebench launch_batch\n",
      "\n",
      " - Using non-default option(s):\n",
      "   >mccebench launch_batch -n_active <jobs to maintain>\n",
      "   >mccebench launch_batch -job_name <my_job_name> -sentinel_file step2_out.pdb\n"
     ]
    }
   ],
   "source": [
    "cli_parser.print_usage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Description:\n",
      "Launch a MCCE benchmarking job using curated structures from the pKa Database v1.\n",
      "\n",
      "The main command is 'mccebench' along with one of 3 sub-commands:\n",
      "- Sub-command 1: 'data_setup': setup data folders;\n",
      "- Sub-command 2: 'script_setup': setup the run script to run mcce steps 1 through 4;\n",
      "- Sub-command 3: 'launch_batch': launch a batch of jobs;\n",
      "\n",
      "Post an issue for all errors and feature requests at:\n",
      "https://github.com/GunnerLab/MCCE_Benchmarking/issues\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(cli_parser.description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['data_setup', '-benchmarks_dir', '/home/cat/projects/mcce_benchmarks']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO]: mcce_benchmark.cli, bench_data_setup:\n",
      "\tmccebench args:\n",
      "{'subparser_name': 'data_setup',\n",
      " 'benchmarks_dir': PosixPath('/home/cat/projects/mcce_benchmarks'),\n",
      " 'func': <function mcce_benchmark.cli.bench_data_setup(args: argparse.Namespace)>}\n",
      "\n",
      "[INFO]: mcce_benchmark.cli, bench_data_setup:\n",
      "\tPreparing pdbs folder & data in /home/cat/projects/mcce_benchmarks.\n",
      "[INFO]: mcce_benchmark.job_setup, setup_pdbs_folder:\n",
      "\tuser_pdbs_folder = PosixPath('/home/cat/projects/mcce_benchmarks/clean_pdbs')\n",
      "[INFO]: mcce_benchmark.audit, list_all_valid_pdbs:\n",
      "\tlen(valid) = 139; len(invalid) = 0\n",
      "[INFO]: mcce_benchmark.job_setup, setup_pdbs_folder:\n",
      "\tCopied .pdb.full for 1ANS\n",
      "[INFO]: mcce_benchmark.job_setup, setup_pdbs_folder:\n",
      "\tCopied .pdb.full for 1BEG\n",
      "[INFO]: mcce_benchmark.job_setup, setup_pdbs_folder:\n",
      "\tCopied .pdb.full for 1BNR\n",
      "[INFO]: mcce_benchmark.job_setup, setup_pdbs_folder:\n",
      "\tCopied .pdb.full for 1BUS\n",
      "[INFO]: mcce_benchmark.job_setup, setup_pdbs_folder:\n",
      "\tCopied .pdb.full for 1CVO\n",
      "[INFO]: mcce_benchmark.job_setup, setup_pdbs_folder:\n",
      "\tCopied .pdb.full for 1DE3\n",
      "[INFO]: mcce_benchmark.job_setup, setup_pdbs_folder:\n",
      "\tCopied .pdb.full for 1EGF\n",
      "[INFO]: mcce_benchmark.job_setup, setup_pdbs_folder:\n",
      "\tCopied .pdb.full for 1EPH\n",
      "[INFO]: mcce_benchmark.job_setup, setup_pdbs_folder:\n",
      "\tCopied .pdb.full for 1FW7\n",
      "[INFO]: mcce_benchmark.job_setup, setup_pdbs_folder:\n",
      "\tCopied .pdb.full for 1GB1\n",
      "[INFO]: mcce_benchmark.job_setup, setup_pdbs_folder:\n",
      "\tCopied .pdb.full for 1HIC\n",
      "[INFO]: mcce_benchmark.job_setup, setup_pdbs_folder:\n",
      "\tCopied .pdb.full for 1JAS\n",
      "[INFO]: mcce_benchmark.job_setup, setup_pdbs_folder:\n",
      "\tCopied .pdb.full for 1MEK\n",
      "[INFO]: mcce_benchmark.job_setup, setup_pdbs_folder:\n",
      "\tCopied .pdb.full for 1MUT\n",
      "[INFO]: mcce_benchmark.job_setup, setup_pdbs_folder:\n",
      "\tCopied .pdb.full for 1NZP\n",
      "[INFO]: mcce_benchmark.job_setup, setup_pdbs_folder:\n",
      "\tCopied .pdb.full for 1YGW\n",
      "[INFO]: mcce_benchmark.job_setup, setup_pdbs_folder:\n",
      "\tCopied .pdb.full for 2IGH\n",
      "[INFO]: mcce_benchmark.job_setup, setup_pdbs_folder:\n",
      "\tCopied .pdb.full for 3EGF\n",
      "[INFO]: mcce_benchmark.job_setup, setup_pdbs_folder:\n",
      "\tCopied .pdb.full for 3GB1\n",
      "[INFO]: mcce_benchmark.job_setup, setup_pdbs_folder:\n",
      "\tNext: Validity check on user data.\n",
      "[INFO]: mcce_benchmark.audit, list_all_valid_pdbs:\n",
      "\tlen(valid) = 139; len(invalid) = 0\n",
      "[INFO]: mcce_benchmark.job_setup, setup_pdbs_folder:\n",
      "\tThe data setup in /home/cat/projects/mcce_benchmarks/clean_pdbs went beautifully!\n",
      "[INFO]: mcce_benchmark.cli, bench_data_setup:\n",
      "\tSetup over.\n"
     ]
    }
   ],
   "source": [
    "# Warning: Do not rerun if you have already setup your <benchmark_dir> with a reduced set:\n",
    "# it will be replaced with the full set.\n",
    "\n",
    "# exclude 'mccebench' when testing:\n",
    "cmd = f\"data_setup -benchmarks_dir {benchmarks_dir}\".split()\n",
    "cmd\n",
    "args = cli_parser.parse_args(cmd)\n",
    "args.func(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['script_setup',\n",
       " '-benchmarks_dir',\n",
       " '/home/cat/projects/mcce_benchmarks',\n",
       " '-job_name',\n",
       " 'foo']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO]: mcce_benchmark.cli, bench_script_setup:\n",
      "\tmccebench args:\n",
      "{'subparser_name': 'script_setup',\n",
      " 'benchmarks_dir': PosixPath('/home/cat/projects/mcce_benchmarks'),\n",
      " 'job_name': 'foo',\n",
      " 'dry': False,\n",
      " 'norun': False,\n",
      " 'e': 'mcce',\n",
      " 'eps': 4,\n",
      " 'u': '',\n",
      " 'func': <function mcce_benchmark.cli.bench_script_setup(args: argparse.Namespace) -> None>}\n",
      "\n",
      "[INFO]: mcce_benchmark.cli, bench_script_setup:\n",
      "\tWrite fresh book file.\n",
      "[INFO]: mcce_benchmark.cli, bench_script_setup:\n",
      "\tWriting script for 'foo' job.\n",
      "[INFO]: mcce_benchmark.job_setup, get_default_script:\n",
      "\tRe-installed default_run.sh\n",
      "[INFO]: mcce_benchmark.job_setup, write_run_script:\n",
      "\tSoft-linked default_run.sh as foo.sh\n",
      "[INFO]: mcce_benchmark.job_setup, write_run_script:\n",
      "\tScript contents:\n",
      "#!/bin/bash\n",
      "\n",
      "step1.py --dry prot.pdb\n",
      "step2.py -d 4\n",
      "step3.py -d 4\n",
      "step4.py\n",
      "\n",
      "sleep 10\n",
      "\n",
      "[INFO]: mcce_benchmark.cli, bench_script_setup:\n",
      "\tDeleting previous pK.out files, if any.\n",
      "[INFO]: mcce_benchmark.job_setup, delete_pkout:\n",
      "\t0 pK.out file(s) deleted.\n"
     ]
    }
   ],
   "source": [
    "job = \"foo\"\n",
    "cmd = f\"script_setup -benchmarks_dir {benchmarks_dir} -job_name {job}\".split()\n",
    "cmd\n",
    "args = cli_parser.parse_args(cmd)\n",
    "args.func(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['launch_batch',\n",
       " '-benchmarks_dir',\n",
       " '/home/cat/projects/mcce_benchmarks',\n",
       " '-job_name',\n",
       " 'dummy',\n",
       " '-n_active',\n",
       " '3']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO]: mcce_benchmark.cli, bench_launch_batch:\n",
      "\tmccebench args:\n",
      "{'subparser_name': 'launch_batch',\n",
      " 'benchmarks_dir': PosixPath('/home/cat/projects/mcce_benchmarks'),\n",
      " 'job_name': 'dummy',\n",
      " 'n_active': 3,\n",
      " 'sentinel_file': 'pK.out',\n",
      " 'func': <function mcce_benchmark.cli.bench_launch_batch(args: argparse.Namespace) -> None>}\n",
      "\n",
      "[INFO]: mcce_benchmark.cli, bench_launch_batch:\n",
      "\tSubmiting batch of jobs.\n",
      "[INFO]: mcce_benchmark.cli, bench_launch_batch:\n",
      "\tDebug mode: batch_submit.launch_job not called\n"
     ]
    }
   ],
   "source": [
    "# exclude 'mccebench' when testing in nbk:\n",
    "n_jobs_to_maintain = 3\n",
    "\n",
    "cmd = f\"launch_batch -benchmarks_dir {benchmarks_dir} -job_name {job} -n_active {n_jobs_to_maintain}\".split()\n",
    "cmd\n",
    "args = cli_parser.parse_args(cmd)\n",
    "args.func(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "\n",
    "# Packaged data management\n",
    "## Prep of the \"master\" pdbs folder, `BENCH_PDBS`:\n",
    " * Remove any MCCE output files or folder along with prot.pdb\n",
    "---"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from benchmark import audit, cleanup\n",
    "\n",
    "cleanup.clean_job_folder(BENCH.BENCH_PDBS)\n",
    "audit.reset_book_file(BENCH.BENCH_Q_BOOK)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "def main():\n",
    "    \"\"\"main fn of pkanalysis.py\"\"\"\n",
    "\n",
    "    calc_pkas = job_pkas_to_dict()\n",
    "    expr_pkas = experimental_pkas_to_dict()\n",
    "    matched_pks = match_pkas(expr_pkas, calc_pkas)\n",
    "    n = len(matched_pks)\n",
    "    matched_pkas_to_csv(matched_pks)\n",
    "\n",
    "    # Overall fitting\n",
    "    x = np.array([p[1] for p in matched_pks])  # x: experiemntal pKas\n",
    "    y = np.array([p[2] for p in matched_pks])  # y: calculated pKas\n",
    "\n",
    "    b, m = np.polynomial.Polynomial.fit(x, y, 1, domain=[0,20]).convert().coef\n",
    "    op = \"+\" if m > 0 else \"-\"\n",
    "    print(f\"y (calculated pKa) = {b:.3f} {op} {abs(m):.3f}x (experimental pKa)\")\n",
    "\n",
    "    delta = x - y\n",
    "    rmsd = np.sqrt(np.mean(delta**2))\n",
    "    print(f\"RMSD between expl. and calc. = {rmsd:.3f}\")\n",
    "\n",
    "    within_2, within_1 = 0, 0\n",
    "    for d in np.abs(delta):\n",
    "        if d <= 2.0:\n",
    "            within_2 += 1\n",
    "            if d <= 1.0:\n",
    "                within_1 += 1\n",
    "\n",
    "    print(f\"{within_2/n:.1%} within 2 pH units\")\n",
    "    print(f\"{within_1/n:.1%} within 1 pH unit\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matched_pks = []\n",
    "for i in range(10):\n",
    "    matched_pks.append((random.choice(\"ABCDRGWSX\"),\n",
    "                         random.choice([3.2, 5.1, 6., 4.4, 7.2]),\n",
    "                        random.choice([3.2, 5.1, 6., 4.4, 7.2]*2)))\n",
    "matched_pks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pka_dict = experimental_pkas_to_dict(WT)\n",
    "len(pka_dict)\n",
    "list(pka_dict.keys())[:10]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:mce]",
   "language": "python",
   "name": "conda-env-mce-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
